# Kubernetesæœ¬ç•ªç’°å¢ƒ ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é‹ç”¨å®Œå…¨ã‚¬ã‚¤ãƒ‰

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€Kubernetesæœ¬ç•ªç’°å¢ƒã§å¿…é ˆã¨ãªã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é‹ç”¨ã‚’ã€æŠ€è¡“çš„åŸç†ã‹ã‚‰å®Ÿè·µã¾ã§å¾¹åº•çš„ã«è§£èª¬ã—ã¾ã™ã€‚

## ğŸ“š ç›®æ¬¡

1. [ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰](#1-ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰)
2. [ãƒãƒ¼ãƒ‰ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹](#2-ãƒãƒ¼ãƒ‰ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹)
3. [è¨¼æ˜æ›¸ç®¡ç†](#3-è¨¼æ˜æ›¸ç®¡ç†)
4. [å®¹é‡è¨ˆç”»](#4-å®¹é‡è¨ˆç”»)

---

## 1. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰

### 1.1 Kubernetes Version Upgrade æˆ¦ç•¥

#### ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚¹ã‚­ãƒ¥ãƒ¼ãƒãƒªã‚·ãƒ¼

```
Kubernetesã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›æ€§:

kube-apiserver: v1.28.x  â† åŸºæº–ãƒãƒ¼ã‚¸ãƒ§ãƒ³
       â”‚
       â”œâ”€ kube-controller-manager: v1.28.x ã¾ãŸã¯ v1.27.x
       â”œâ”€ kube-scheduler:          v1.28.x ã¾ãŸã¯ v1.27.x
       â”œâ”€ kubelet:                 v1.28.x, v1.27.x, ã¾ãŸã¯ v1.26.x
       â”œâ”€ kube-proxy:              v1.28.x, v1.27.x, ã¾ãŸã¯ v1.26.x
       â””â”€ kubectl:                 v1.29.x, v1.28.x, ã¾ãŸã¯ v1.27.x

ãƒ«ãƒ¼ãƒ«:
1. kube-apiserver ãŒæœ€æ–°ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹
2. controller-manager, scheduler ã¯ apiserver ã® Â±1 ãƒãƒ¼ã‚¸ãƒ§ãƒ³
3. kubelet, kube-proxy ã¯ apiserver ã® -2 ã¾ã§ã‚µãƒãƒ¼ãƒˆ
4. kubectl ã¯ apiserver ã® Â±1 ãƒãƒ¼ã‚¸ãƒ§ãƒ³

âš ï¸ ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚¹ã‚­ãƒƒãƒ—ã¯éæ¨å¥¨ âš ï¸
ä¾‹: v1.26 â†’ v1.28 ã¯ NG
     v1.26 â†’ v1.27 â†’ v1.28 ã¨æ®µéšçš„ã«ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
```

### 1.2 ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰æ‰‹é †ï¼ˆkubeadmï¼‰

#### ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒ³ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰

```bash
#!/bin/bash
# upgrade-control-plane.sh - ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒãƒ¼ãƒ‰ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰

set -euo pipefail

CURRENT_VERSION="1.27.8"
TARGET_VERSION="1.28.4"

echo "=== Kubernetes Control Plane Upgrade ==="
echo "Current version: ${CURRENT_VERSION}"
echo "Target version:  ${TARGET_VERSION}"
echo ""

# 1. æœ€æ–°ã®kubeadmã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
echo "Step 1: Upgrading kubeadm..."
apt-mark unhold kubeadm
apt-get update
apt-get install -y kubeadm=${TARGET_VERSION}-00
apt-mark hold kubeadm

# kubeadm ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
kubeadm version

# 2. ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ—ãƒ©ãƒ³ã®ç¢ºèª
echo "Step 2: Checking upgrade plan..."
kubeadm upgrade plan

# å‡ºåŠ›ä¾‹:
# [upgrade/config] Making sure the configuration is correct:
# [upgrade/config] Reading configuration from the cluster...
# [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
# [preflight] Running pre-flight checks.
# [upgrade] Running cluster health checks
# [upgrade] Fetching available versions to upgrade to
# [upgrade/versions] Cluster version: v1.27.8
# [upgrade/versions] kubeadm version: v1.28.4
# [upgrade/versions] Target version: v1.28.4
#
# Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
# COMPONENT   CURRENT       TARGET
# kubelet     3 x v1.27.8   v1.28.4
#
# Upgrade to the latest version in the v1.28 series:
#
# COMPONENT                 CURRENT   TARGET
# kube-apiserver            v1.27.8   v1.28.4
# kube-controller-manager   v1.27.8   v1.28.4
# kube-scheduler            v1.27.8   v1.28.4
# kube-proxy                v1.27.8   v1.28.4
# CoreDNS                   v1.10.1   v1.10.1
# etcd                      3.5.9-0   3.5.9-0

echo ""
read -p "Proceed with upgrade? (yes/no): " confirm
if [ "$confirm" != "yes" ]; then
    echo "Upgrade cancelled"
    exit 1
fi

# 3. æœ€åˆã®ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒãƒ¼ãƒ‰ã‚’ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
echo "Step 3: Applying upgrade..."
kubeadm upgrade apply v${TARGET_VERSION} -y

# 4. CNIãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
echo "Step 4: Upgrading CNI..."
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml

# 5. ãƒãƒ¼ãƒ‰ã®drainï¼ˆPodã®é€€é¿ï¼‰
echo "Step 5: Draining node..."
kubectl drain $(hostname) --ignore-daemonsets --delete-emptydir-data

# 6. kubelet ã¨ kubectl ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
echo "Step 6: Upgrading kubelet and kubectl..."
apt-mark unhold kubelet kubectl
apt-get update
apt-get install -y kubelet=${TARGET_VERSION}-00 kubectl=${TARGET_VERSION}-00
apt-mark hold kubelet kubectl

# 7. kubelet ã®å†èµ·å‹•
echo "Step 7: Restarting kubelet..."
systemctl daemon-reload
systemctl restart kubelet

# 8. ãƒãƒ¼ãƒ‰ã®uncordonï¼ˆPodã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å†é–‹ï¼‰
echo "Step 8: Uncordoning node..."
kubectl uncordon $(hostname)

# 9. ç¢ºèª
echo "Step 9: Verification..."
kubectl get nodes
kubectl version

echo ""
echo "=== Control plane upgrade completed ==="
echo "Next steps:"
echo "1. Verify cluster functionality"
echo "2. Upgrade other control plane nodes (if HA)"
echo "3. Upgrade worker nodes"
```

#### ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰

```bash
#!/bin/bash
# upgrade-worker.sh - ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰

set -euo pipefail

TARGET_VERSION="1.28.4"
NODE_NAME=$(hostname)

echo "=== Worker Node Upgrade: ${NODE_NAME} ==="

# 1. ãƒãƒ¼ãƒ‰ã®drainï¼ˆãƒã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰ã‹ã‚‰å®Ÿè¡Œï¼‰
echo "Step 1: Draining node ${NODE_NAME}..."
# ã“ã®æ‰‹é †ã¯ãƒã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰ã‹ã‚‰å®Ÿè¡Œ:
# kubectl drain ${NODE_NAME} --ignore-daemonsets --delete-emptydir-data --force --grace-period=300

# PodDisruptionBudget ã‚’è€ƒæ…®ã—ãŸdrain
# kubectl drain ${NODE_NAME} \
#   --ignore-daemonsets \
#   --delete-emptydir-data \
#   --force \
#   --grace-period=300 \
#   --timeout=600s \
#   --pod-selector='app!=critical-service'

# 2. kubeadm ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
echo "Step 2: Upgrading kubeadm..."
apt-mark unhold kubeadm
apt-get update
apt-get install -y kubeadm=${TARGET_VERSION}-00
apt-mark hold kubeadm

# 3. ãƒãƒ¼ãƒ‰ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
echo "Step 3: Upgrading node configuration..."
kubeadm upgrade node

# 4. kubelet ã¨ kubectl ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
echo "Step 4: Upgrading kubelet and kubectl..."
apt-mark unhold kubelet kubectl
apt-get update
apt-get install -y kubelet=${TARGET_VERSION}-00 kubectl=${TARGET_VERSION}-00
apt-mark hold kubelet kubectl

# 5. kubelet ã®å†èµ·å‹•
echo "Step 5: Restarting kubelet..."
systemctl daemon-reload
systemctl restart kubelet

# 6. ãƒãƒ¼ãƒ‰ã®uncordonï¼ˆãƒã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰ã‹ã‚‰å®Ÿè¡Œï¼‰
echo "Step 6: Uncordoning node ${NODE_NAME}..."
# ã“ã®æ‰‹é †ã¯ãƒã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰ã‹ã‚‰å®Ÿè¡Œ:
# kubectl uncordon ${NODE_NAME}

echo ""
echo "=== Worker node upgrade completed ==="
```

### 1.3 ãƒãƒãƒ¼ã‚¸ãƒ‰Kubernetesã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ï¼ˆEKSä¾‹ï¼‰

```bash
#!/bin/bash
# eks-upgrade.sh - EKSã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰

set -euo pipefail

CLUSTER_NAME="production-eks"
CURRENT_VERSION="1.27"
TARGET_VERSION="1.28"
REGION="ap-northeast-1"

echo "=== EKS Cluster Upgrade ==="
echo "Cluster: ${CLUSTER_NAME}"
echo "Current version: ${CURRENT_VERSION}"
echo "Target version: ${TARGET_VERSION}"

# 1. äº‹å‰ç¢ºèª
echo "Step 1: Pre-upgrade checks..."

# ã‚¢ãƒ‰ã‚ªãƒ³ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
aws eks describe-addon-versions \
    --kubernetes-version ${TARGET_VERSION} \
    --region ${REGION}

# éæ¨å¥¨APIã®ç¢ºèª
kubectl get --raw /metrics | grep apiserver_requested_deprecated_apis

# 2. ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒ³ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
echo "Step 2: Upgrading EKS control plane..."
aws eks update-cluster-version \
    --name ${CLUSTER_NAME} \
    --kubernetes-version ${TARGET_VERSION} \
    --region ${REGION}

# ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã®é€²è¡ŒçŠ¶æ³ã‚’ç›£è¦–
while true; do
    STATUS=$(aws eks describe-update \
        --name ${CLUSTER_NAME} \
        --update-id $(aws eks list-updates --name ${CLUSTER_NAME} --region ${REGION} --query 'updateIds[0]' --output text) \
        --region ${REGION} \
        --query 'update.status' \
        --output text)

    echo "Update status: ${STATUS}"

    if [ "${STATUS}" = "Successful" ]; then
        echo "Control plane upgrade completed"
        break
    elif [ "${STATUS}" = "Failed" ]; then
        echo "Control plane upgrade failed"
        exit 1
    fi

    sleep 30
done

# 3. ã‚¢ãƒ‰ã‚ªãƒ³ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
echo "Step 3: Upgrading EKS addons..."

# kube-proxy
aws eks update-addon \
    --cluster-name ${CLUSTER_NAME} \
    --addon-name kube-proxy \
    --addon-version $(aws eks describe-addon-versions --kubernetes-version ${TARGET_VERSION} --addon-name kube-proxy --query 'addons[0].addonVersions[0].addonVersion' --output text) \
    --region ${REGION}

# CoreDNS
aws eks update-addon \
    --cluster-name ${CLUSTER_NAME} \
    --addon-name coredns \
    --addon-version $(aws eks describe-addon-versions --kubernetes-version ${TARGET_VERSION} --addon-name coredns --query 'addons[0].addonVersions[0].addonVersion' --output text) \
    --region ${REGION}

# VPC CNI
aws eks update-addon \
    --cluster-name ${CLUSTER_NAME} \
    --addon-name vpc-cni \
    --addon-version $(aws eks describe-addon-versions --kubernetes-version ${TARGET_VERSION} --addon-name vpc-cni --query 'addons[0].addonVersions[0].addonVersion' --output text) \
    --region ${REGION}

# 4. ãƒãƒ¼ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
echo "Step 4: Upgrading node groups..."

# å„ãƒãƒ¼ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—ã‚’é †æ¬¡ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
for NODE_GROUP in $(aws eks list-nodegroups --cluster-name ${CLUSTER_NAME} --region ${REGION} --query 'nodegroups[]' --output text); do
    echo "Upgrading node group: ${NODE_GROUP}"

    # ãƒãƒ¼ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
    aws eks update-nodegroup-version \
        --cluster-name ${CLUSTER_NAME} \
        --nodegroup-name ${NODE_GROUP} \
        --region ${REGION}

    # å®Œäº†ã‚’å¾…ã¤
    aws eks wait nodegroup-active \
        --cluster-name ${CLUSTER_NAME} \
        --nodegroup-name ${NODE_GROUP} \
        --region ${REGION}

    echo "Node group ${NODE_GROUP} upgraded"
done

# 5. ç¢ºèª
echo "Step 5: Verification..."
kubectl version
kubectl get nodes

echo ""
echo "=== EKS cluster upgrade completed ==="
```

---

## 2. ãƒãƒ¼ãƒ‰ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹

### 2.1 Drain ã¨ Cordon

#### Drainï¼ˆPodã®å®‰å…¨ãªé€€é¿ï¼‰

```bash
# åŸºæœ¬çš„ãª drain
kubectl drain node-1 --ignore-daemonsets

# ã‚ˆã‚Šè©³ç´°ãªè¨­å®š
kubectl drain node-1 \
  --ignore-daemonsets \              # DaemonSet Podã¯ç„¡è¦–
  --delete-emptydir-data \           # emptyDir ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹Podã‚‚å‰Šé™¤
  --force \                          # ReplicaSetã«ç®¡ç†ã•ã‚Œã¦ã„ãªã„Podã‚‚å¼·åˆ¶å‰Šé™¤
  --grace-period=300 \               # çµ‚äº†çŒ¶äºˆæœŸé–“ï¼ˆç§’ï¼‰
  --timeout=600s \                   # drainæ“ä½œã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
  --pod-selector='tier!=critical'    # ç‰¹å®šã®Podã®ã¿drain

# Drain ãŒå¤±æ•—ã™ã‚‹ä¸€èˆ¬çš„ãªã‚±ãƒ¼ã‚¹:
# 1. PodDisruptionBudget (PDB) ã«ã‚ˆã‚Šã€æœ€å°ãƒ¬ãƒ—ãƒªã‚«æ•°ã‚’ä¸‹å›ã‚‹
# 2. local storage (emptyDir) ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹Pod
# 3. Standalone Podï¼ˆReplicaSetã«ç®¡ç†ã•ã‚Œã¦ã„ãªã„ï¼‰

# PDB ã‚’ç¢ºèª
kubectl get pdb --all-namespaces

# ä¾‹:
# NAMESPACE   NAME           MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
# prod        myapp-pdb      2               N/A               0                     30d
```

#### Cordonï¼ˆæ–°è¦Podã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«åœæ­¢ï¼‰

```bash
# ãƒãƒ¼ãƒ‰ã¸ã®æ–°è¦Podã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’åœæ­¢ï¼ˆæ—¢å­˜Podã¯æ®‹ã‚‹ï¼‰
kubectl cordon node-1

# è¤‡æ•°ãƒãƒ¼ãƒ‰ã‚’ä¸€æ‹¬cordon
kubectl cordon node-1 node-2 node-3

# ç¢ºèª
kubectl get nodes
# NAME     STATUS                     ROLES    AGE   VERSION
# node-1   Ready,SchedulingDisabled   <none>   30d   v1.28.0

# Uncordonï¼ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å†é–‹ï¼‰
kubectl uncordon node-1
```

### 2.2 OS Patching

#### ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆæ–¹å¼

```bash
#!/bin/bash
# rolling-os-patch.sh - ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ã®OSãƒ‘ãƒƒãƒã‚’ãƒ­ãƒ¼ãƒªãƒ³ã‚°é©ç”¨

set -euo pipefail

NODES=$(kubectl get nodes -l node-role.kubernetes.io/worker=true -o name | sed 's/node\///')

for NODE in ${NODES}; do
    echo "=== Patching node: ${NODE} ==="

    # 1. Cordon
    echo "Cordoning ${NODE}..."
    kubectl cordon ${NODE}

    # 2. Drain
    echo "Draining ${NODE}..."
    kubectl drain ${NODE} \
        --ignore-daemonsets \
        --delete-emptydir-data \
        --force \
        --grace-period=300 \
        --timeout=600s

    # 3. OSãƒ‘ãƒƒãƒã®é©ç”¨ï¼ˆSSHçµŒç”±ï¼‰
    echo "Applying OS patches on ${NODE}..."
    ssh ${NODE} sudo apt-get update
    ssh ${NODE} sudo apt-get upgrade -y

    # ã‚«ãƒ¼ãƒãƒ«ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆãŒã‚ã‚‹å ´åˆã¯å†èµ·å‹•
    if ssh ${NODE} '[ -f /var/run/reboot-required ]'; then
        echo "Reboot required for ${NODE}"
        ssh ${NODE} sudo reboot

        # ãƒãƒ¼ãƒ‰ãŒã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã«ãªã‚‹ã¾ã§å¾…æ©Ÿ
        sleep 30

        # ãƒãƒ¼ãƒ‰ãŒã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã«æˆ»ã‚‹ã¾ã§å¾…æ©Ÿ
        until ssh ${NODE} uptime 2>/dev/null; do
            echo "Waiting for ${NODE} to come back online..."
            sleep 10
        done

        # kubeletãŒèµ·å‹•ã™ã‚‹ã¾ã§å¾…æ©Ÿ
        until kubectl get node ${NODE} | grep -q Ready; do
            echo "Waiting for kubelet on ${NODE}..."
            sleep 10
        done
    fi

    # 4. Uncordon
    echo "Uncordoning ${NODE}..."
    kubectl uncordon ${NODE}

    # 5. PodãŒæ­£å¸¸ã«ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
    sleep 60

    # 6. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®å¥å…¨æ€§ç¢ºèª
    echo "Checking cluster health..."
    kubectl get nodes
    kubectl get pods --all-namespaces | grep -v Running | grep -v Completed || echo "All pods are running"

    echo "Node ${NODE} patching completed"
    echo ""
done

echo "=== All nodes patched successfully ==="
```

### 2.3 Node Problem Detector

#### Node Problem Detector ã®ãƒ‡ãƒ—ãƒ­ã‚¤

```yaml
# Node Problem Detector DaemonSet
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-problem-detector
  namespace: kube-system
  labels:
    app: node-problem-detector
spec:
  selector:
    matchLabels:
      app: node-problem-detector
  template:
    metadata:
      labels:
        app: node-problem-detector
    spec:
      # ã™ã¹ã¦ã®ãƒãƒ¼ãƒ‰ã«é…ç½®
      tolerations:
      - effect: NoSchedule
        operator: Exists

      hostNetwork: true

      containers:
      - name: node-problem-detector
        image: registry.k8s.io/node-problem-detector/node-problem-detector:v0.8.13
        command:
        - /node-problem-detector
        - --logtostderr
        - --config.system-log-monitor=/config/kernel-monitor.json
        - --config.system-log-monitor=/config/docker-monitor.json
        - --config.custom-plugin-monitor=/config/custom-plugin-monitor.json

        securityContext:
          privileged: true

        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName

        volumeMounts:
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹
        - name: log
          mountPath: /var/log
          readOnly: true
        - name: kmsg
          mountPath: /dev/kmsg
          readOnly: true
        # ãƒ›ã‚¹ãƒˆã®æ™‚åˆ»ã‚’ä½¿ç”¨
        - name: localtime
          mountPath: /etc/localtime
          readOnly: true
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
        - name: config
          mountPath: /config
          readOnly: true

      volumes:
      - name: log
        hostPath:
          path: /var/log/
      - name: kmsg
        hostPath:
          path: /dev/kmsg
      - name: localtime
        hostPath:
          path: /etc/localtime
      - name: config
        configMap:
          name: node-problem-detector-config

---
# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
apiVersion: v1
kind: ConfigMap
metadata:
  name: node-problem-detector-config
  namespace: kube-system
data:
  kernel-monitor.json: |
    {
      "plugin": "kmsg",
      "logPath": "/dev/kmsg",
      "lookback": "5m",
      "bufferSize": 10,
      "source": "kernel-monitor",
      "conditions": [
        {
          "type": "KernelDeadlock",
          "reason": "KernelHasNoDeadlock",
          "message": "kernel has no deadlock"
        },
        {
          "type": "ReadonlyFilesystem",
          "reason": "FilesystemIsReadOnly",
          "message": "Filesystem is read-only"
        }
      ],
      "rules": [
        {
          "type": "temporary",
          "reason": "OOMKilling",
          "pattern": "Kill process \\d+ (.+) score \\d+ or sacrifice child\\nKilled process \\d+ (.+) total-vm:\\d+kB, anon-rss:\\d+kB, file-rss:\\d+kB.*"
        },
        {
          "type": "permanent",
          "condition": "KernelDeadlock",
          "reason": "TaskHung",
          "pattern": "task \\S+:\\w+ blocked for more than \\w+ seconds\\."
        },
        {
          "type": "permanent",
          "condition": "ReadonlyFilesystem",
          "reason": "FilesystemReadonly",
          "pattern": "Remounting filesystem read-only"
        }
      ]
    }

  docker-monitor.json: |
    {
      "plugin": "journald",
      "pluginConfig": {
        "source": "dockerd"
      },
      "logPath": "/var/log/journal",
      "lookback": "5m",
      "bufferSize": 10,
      "source": "docker-monitor",
      "conditions": [],
      "rules": [
        {
          "type": "temporary",
          "reason": "CorruptDockerImage",
          "pattern": "Error trying v2 registry: failed to register layer: rename /var/lib/docker/image/(.+) /var/lib/docker/image/(.+): directory not empty.*"
        }
      ]
    }

  custom-plugin-monitor.json: |
    {
      "plugin": "custom",
      "pluginConfig": {
        "invoke_interval": "30s",
        "timeout": "5s",
        "max_output_length": 80,
        "concurrency": 3
      },
      "source": "custom-plugin-monitor",
      "conditions": [
        {
          "type": "FrequentUnregisterNetDevice",
          "reason": "NoFrequentUnregisterNetDevice",
          "message": "node is functioning properly"
        }
      ],
      "rules": [
        {
          "type": "permanent",
          "condition": "FrequentUnregisterNetDevice",
          "reason": "UnregisterNetDevice",
          "path": "/config/plugin/check_unregister_net_device.sh",
          "timeout": "3s"
        }
      ]
    }
```

#### Node Conditionã®ç¢ºèª

```bash
# ãƒãƒ¼ãƒ‰ã®çŠ¶æ…‹ç¢ºèª
kubectl describe node node-1

# å‡ºåŠ›ä¾‹ï¼ˆConditionséƒ¨åˆ†ï¼‰:
# Conditions:
#   Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message
#   ----                          ------  -----------------                 ------------------                ------                          -------
#   FrequentUnregisterNetDevice   False   Mon, 15 Jan 2024 10:30:00 +0900   Mon, 15 Jan 2024 08:00:00 +0900   NoFrequentUnregisterNetDevice   node is functioning properly
#   KernelDeadlock                False   Mon, 15 Jan 2024 10:30:00 +0900   Mon, 15 Jan 2024 08:00:00 +0900   KernelHasNoDeadlock             kernel has no deadlock
#   ReadonlyFilesystem            False   Mon, 15 Jan 2024 10:30:00 +0900   Mon, 15 Jan 2024 08:00:00 +0900   FilesystemIsNotReadOnly         Filesystem is not read-only
#   MemoryPressure                False   Mon, 15 Jan 2024 10:30:15 +0900   Mon, 15 Jan 2024 08:00:00 +0900   KubeletHasSufficientMemory      kubelet has sufficient memory available
#   DiskPressure                  False   Mon, 15 Jan 2024 10:30:15 +0900   Mon, 15 Jan 2024 08:00:00 +0900   KubeletHasNoDiskPressure        kubelet has no disk pressure
#   PIDPressure                   False   Mon, 15 Jan 2024 10:30:15 +0900   Mon, 15 Jan 2024 08:00:00 +0900   KubeletHasSufficientPID         kubelet has sufficient PID available
#   Ready                         True    Mon, 15 Jan 2024 10:30:15 +0900   Mon, 15 Jan 2024 08:00:00 +0900   KubeletReady                    kubelet is posting ready status

# å•é¡Œã®ã‚ã‚‹ãƒãƒ¼ãƒ‰ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
kubectl get nodes -o json | jq '.items[] | select(.status.conditions[] | select(.type=="Ready" and .status!="True")) | .metadata.name'
```

---

## 3. è¨¼æ˜æ›¸ç®¡ç†

### 3.1 Kubernetes è¨¼æ˜æ›¸ã®æ§‹é€ 

#### è¨¼æ˜æ›¸ã®ç¨®é¡ã¨ç”¨é€”

```
Kubernetesè¨¼æ˜æ›¸ã®éšå±¤æ§‹é€ :

/etc/kubernetes/pki/
â”œâ”€â”€ ca.crt                              # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼CAï¼ˆroot CAï¼‰
â”œâ”€â”€ ca.key                              # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼CAç§˜å¯†éµ
â”œâ”€â”€ apiserver.crt                       # kube-apiserver ã‚µãƒ¼ãƒãƒ¼è¨¼æ˜æ›¸
â”œâ”€â”€ apiserver.key                       # kube-apiserver ç§˜å¯†éµ
â”œâ”€â”€ apiserver-kubelet-client.crt        # apiserver â†’ kubelet ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨¼æ˜æ›¸
â”œâ”€â”€ apiserver-kubelet-client.key        # apiserver â†’ kubelet ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç§˜å¯†éµ
â”œâ”€â”€ front-proxy-ca.crt                  # ãƒ•ãƒ­ãƒ³ãƒˆãƒ—ãƒ­ã‚­ã‚·CA
â”œâ”€â”€ front-proxy-ca.key                  # ãƒ•ãƒ­ãƒ³ãƒˆãƒ—ãƒ­ã‚­ã‚·CAç§˜å¯†éµ
â”œâ”€â”€ front-proxy-client.crt              # ãƒ•ãƒ­ãƒ³ãƒˆãƒ—ãƒ­ã‚­ã‚·ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨¼æ˜æ›¸
â”œâ”€â”€ front-proxy-client.key              # ãƒ•ãƒ­ãƒ³ãƒˆãƒ—ãƒ­ã‚­ã‚·ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç§˜å¯†éµ
â”œâ”€â”€ sa.pub                              # ServiceAccountå…¬é–‹éµ
â”œâ”€â”€ sa.key                              # ServiceAccountç§˜å¯†éµ
â”‚
â””â”€â”€ etcd/
    â”œâ”€â”€ ca.crt                          # etcd CA
    â”œâ”€â”€ ca.key                          # etcd CAç§˜å¯†éµ
    â”œâ”€â”€ server.crt                      # etcd ã‚µãƒ¼ãƒãƒ¼è¨¼æ˜æ›¸
    â”œâ”€â”€ server.key                      # etcd ã‚µãƒ¼ãƒãƒ¼ç§˜å¯†éµ
    â”œâ”€â”€ peer.crt                        # etcd ãƒ”ã‚¢é€šä¿¡è¨¼æ˜æ›¸
    â”œâ”€â”€ peer.key                        # etcd ãƒ”ã‚¢é€šä¿¡ç§˜å¯†éµ
    â”œâ”€â”€ healthcheck-client.crt          # etcd ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨¼æ˜æ›¸
    â””â”€â”€ healthcheck-client.key          # etcd ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç§˜å¯†éµ

è¨¼æ˜æ›¸ã®ç”¨é€”:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ca.crt/ca.key                                            â”‚
â”‚ ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å…¨ä½“ã®ãƒ«ãƒ¼ãƒˆCA                                â”‚
â”‚ ãƒ»ã™ã¹ã¦ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆè¨¼æ˜æ›¸ã‚’ç½²å                      â”‚
â”‚ ãƒ»æœ‰åŠ¹æœŸé™: 10å¹´ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ apiserver.crt/apiserver.key                              â”‚
â”‚ ãƒ»kube-apiserver ã®TLSã‚µãƒ¼ãƒãƒ¼è¨¼æ˜æ›¸                     â”‚
â”‚ ãƒ»ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆkubectl, kubeletç­‰ï¼‰ãŒæ¤œè¨¼               â”‚
â”‚ ãƒ»SAN: kubernetes, kubernetes.default, <å„ç¨®IP/DNS>     â”‚
â”‚ ãƒ»æœ‰åŠ¹æœŸé™: 1å¹´ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰âš ï¸                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ apiserver-kubelet-client.crt/apiserver-kubelet-client.keyâ”‚
â”‚ ãƒ»kube-apiserver ãŒ kubelet ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹éš›ã®           â”‚
â”‚   ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨¼æ˜æ›¸                                     â”‚
â”‚ ãƒ»æœ‰åŠ¹æœŸé™: 1å¹´ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰âš ï¸                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 è¨¼æ˜æ›¸ã®æœ‰åŠ¹æœŸé™ç¢ºèª

```bash
# ã™ã¹ã¦ã®è¨¼æ˜æ›¸ã®æœ‰åŠ¹æœŸé™ã‚’ç¢ºèª
kubeadm certs check-expiration

# å‡ºåŠ›ä¾‹:
# [check-expiration] Reading configuration from the cluster...
# [check-expiration] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
#
# CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED
# admin.conf                 Jan 15, 2025 08:00 UTC   364d            ca                      no
# apiserver                  Jan 15, 2025 08:00 UTC   364d            ca                      no
# apiserver-etcd-client      Jan 15, 2025 08:00 UTC   364d            etcd-ca                 no
# apiserver-kubelet-client   Jan 15, 2025 08:00 UTC   364d            ca                      no
# controller-manager.conf    Jan 15, 2025 08:00 UTC   364d            ca                      no
# etcd-healthcheck-client    Jan 15, 2025 08:00 UTC   364d            etcd-ca                 no
# etcd-peer                  Jan 15, 2025 08:00 UTC   364d            etcd-ca                 no
# etcd-server                Jan 15, 2025 08:00 UTC   364d            etcd-ca                 no
# front-proxy-client         Jan 15, 2025 08:00 UTC   364d            front-proxy-ca          no
# scheduler.conf             Jan 15, 2025 08:00 UTC   364d            ca                      no
#
# CERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED
# ca                      Jan 12, 2034 08:00 UTC   9y              no
# etcd-ca                 Jan 12, 2034 08:00 UTC   9y              no
# front-proxy-ca          Jan 12, 2034 08:00 UTC   9y              no

# å€‹åˆ¥ã®è¨¼æ˜æ›¸ç¢ºèª
openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -text

# æœ‰åŠ¹æœŸé™ã®ã¿è¡¨ç¤º
openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -dates
# notBefore=Jan 15 08:00:00 2024 GMT
# notAfter=Jan 15 08:00:00 2025 GMT
```

### 3.3 è¨¼æ˜æ›¸ã®æ‰‹å‹•æ›´æ–°

```bash
#!/bin/bash
# renew-certificates.sh - Kubernetesè¨¼æ˜æ›¸ã®æ›´æ–°

set -euo pipefail

echo "=== Kubernetes Certificate Renewal ==="

# 1. è¨¼æ˜æ›¸ã®æœ‰åŠ¹æœŸé™ç¢ºèª
echo "Step 1: Checking certificate expiration..."
kubeadm certs check-expiration

# 2. ã™ã¹ã¦ã®è¨¼æ˜æ›¸ã‚’æ›´æ–°
echo "Step 2: Renewing all certificates..."
kubeadm certs renew all

# å‡ºåŠ›ä¾‹:
# [renew] Reading configuration from the cluster...
# [renew] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
#
# certificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed
# certificate for serving the Kubernetes API renewed
# certificate the apiserver uses to access etcd renewed
# certificate for the API server to connect to kubelet renewed
# certificate embedded in the kubeconfig file for the controller manager to use renewed
# certificate for liveness probes to healthcheck etcd renewed
# certificate for etcd nodes to communicate with each other renewed
# certificate for serving etcd renewed
# certificate for the front proxy client renewed
# certificate embedded in the kubeconfig file for the scheduler manager to use renewed

# 3. kubeconfigãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
echo "Step 3: Updating kubeconfig files..."
kubeadm init phase kubeconfig all

# 4. kubeconfigã‚’ãƒ›ãƒ¼ãƒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼
cp /etc/kubernetes/admin.conf ~/.kube/config

# 5. ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒ³ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å†èµ·å‹•
echo "Step 4: Restarting control plane components..."

# é™çš„Podã®å†èµ·å‹•ï¼ˆmanifestã‚’ä¸€æ™‚çš„ã«ç§»å‹•ï¼‰
mv /etc/kubernetes/manifests/kube-apiserver.yaml /tmp/
mv /etc/kubernetes/manifests/kube-controller-manager.yaml /tmp/
mv /etc/kubernetes/manifests/kube-scheduler.yaml /tmp/

sleep 10

mv /tmp/kube-apiserver.yaml /etc/kubernetes/manifests/
mv /tmp/kube-controller-manager.yaml /etc/kubernetes/manifests/
mv /tmp/kube-scheduler.yaml /etc/kubernetes/manifests/

# PodãŒèµ·å‹•ã™ã‚‹ã¾ã§å¾…æ©Ÿ
echo "Waiting for control plane to restart..."
sleep 30

# 6. ç¢ºèª
echo "Step 5: Verification..."
kubeadm certs check-expiration
kubectl get nodes
kubectl get pods -n kube-system

echo ""
echo "=== Certificate renewal completed ==="
```

### 3.4 è¨¼æ˜æ›¸ã®è‡ªå‹•æ›´æ–°ï¼ˆcert-managerï¼‰

```yaml
# cert-manager ã§ã®Kubernetesè¨¼æ˜æ›¸ç®¡ç†
# â€»é€šå¸¸ã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨¼æ˜æ›¸ã«ã¯ä½¿ç”¨ã—ã¾ã›ã‚“ãŒã€Ingressã‚„å†…éƒ¨ã‚µãƒ¼ãƒ“ã‚¹ã®è¨¼æ˜æ›¸ã«ã¯åˆ©ç”¨

---
# cert-manager ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.0/cert-manager.yaml

---
# ClusterIssuer: Let's Encrypt
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: admin@example.com
    privateKeySecretRef:
      name: letsencrypt-prod
    solvers:
    # HTTP-01 challenge
    - http01:
        ingress:
          class: nginx
    # DNS-01 challengeï¼ˆãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰è¨¼æ˜æ›¸ç”¨ï¼‰
    - dns01:
        route53:
          region: ap-northeast-1
          hostedZoneID: Z1234567890ABC
          role: arn:aws:iam::123456789012:role/cert-manager

---
# Certificate: Ingressç”¨ã®è¨¼æ˜æ›¸
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: example-com-tls
  namespace: prod
spec:
  secretName: example-com-tls
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
  commonName: example.com
  dnsNames:
  - example.com
  - www.example.com
  - api.example.com

  # è‡ªå‹•æ›´æ–°è¨­å®š
  renewBefore: 720h  # 30æ—¥å‰ã«æ›´æ–°é–‹å§‹

  # è¨¼æ˜æ›¸ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«
  duration: 2160h    # 90æ—¥ï¼ˆLet's Encryptã®æœ€å¤§æœŸé–“ï¼‰

---
# Ingress with TLS
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
  namespace: prod
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - example.com
    - www.example.com
    secretName: example-com-tls  # cert-managerãŒè‡ªå‹•ä½œæˆ
  rules:
  - host: example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend
            port:
              number: 80
```

---

## 4. å®¹é‡è¨ˆç”»

### 4.1 ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚µã‚¤ã‚¸ãƒ³ã‚°

#### ãƒªã‚½ãƒ¼ã‚¹è¦ä»¶ã®ç®—å‡º

```
å®¹é‡è¨ˆç”»ã®åŸºæœ¬å¼:

å¿…è¦ãƒãƒ¼ãƒ‰æ•° = (ç·Podæ•° Ã— å¹³å‡Podè¦æ±‚ãƒªã‚½ãƒ¼ã‚¹) / (ãƒãƒ¼ãƒ‰å®¹é‡ Ã— ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡)

ä¾‹:
- ç·Podæ•°: 500
- å¹³å‡CPUè¦æ±‚: 0.5 core/pod
- å¹³å‡ãƒ¡ãƒ¢ãƒªè¦æ±‚: 512 MB/pod
- ãƒãƒ¼ãƒ‰å®¹é‡: 8 cores, 32 GB RAM
- ç›®æ¨™ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡: 70%

CPUè¨ˆç®—:
å¿…è¦CPU = 500 pods Ã— 0.5 cores = 250 cores
ãƒãƒ¼ãƒ‰ã‚ãŸã‚ŠCPU = 8 cores Ã— 0.7 = 5.6 cores
å¿…è¦ãƒãƒ¼ãƒ‰æ•°ï¼ˆCPUï¼‰ = 250 / 5.6 = 45 ãƒãƒ¼ãƒ‰

ãƒ¡ãƒ¢ãƒªè¨ˆç®—:
å¿…è¦ãƒ¡ãƒ¢ãƒª = 500 pods Ã— 512 MB = 256 GB
ãƒãƒ¼ãƒ‰ã‚ãŸã‚Šãƒ¡ãƒ¢ãƒª = 32 GB Ã— 0.7 = 22.4 GB
å¿…è¦ãƒãƒ¼ãƒ‰æ•°ï¼ˆãƒ¡ãƒ¢ãƒªï¼‰ = 256 / 22.4 = 12 ãƒãƒ¼ãƒ‰

â†’ CPUåˆ¶ç´„ã®ãŸã‚ã€45ãƒãƒ¼ãƒ‰å¿…è¦

ãƒãƒƒãƒ•ã‚¡ã¨HAè€ƒæ…®:
- N+1å†—é•·æ€§: +1ãƒãƒ¼ãƒ‰
- ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ: +20% (9ãƒãƒ¼ãƒ‰)
- ãƒãƒ¼ã‚¹ãƒˆãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯: +30% (14ãƒãƒ¼ãƒ‰)

æœ€çµ‚çš„ãªæ¨å¥¨ãƒãƒ¼ãƒ‰æ•°: 45 + 1 + 9 + 14 = 69 ãƒãƒ¼ãƒ‰
```

#### å®Ÿéš›ã®ä½¿ç”¨çŠ¶æ³ã®æ¸¬å®š

```bash
# ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å…¨ä½“ã®ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨çŠ¶æ³
kubectl top nodes

# å‡ºåŠ›ä¾‹:
# NAME      CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
# node-1    1200m        15%    8Gi             26%
# node-2    2100m        26%    12Gi            39%
# node-3    890m         11%    6Gi             19%

# Podå˜ä½ã®ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨çŠ¶æ³
kubectl top pods --all-namespaces

# Namespaceå˜ä½ã®ãƒªã‚½ãƒ¼ã‚¹é›†è¨ˆ
kubectl top pods -n prod | awk '{sum+=$2} END {print "Total CPU: " sum "m"}'
kubectl top pods -n prod | awk '{sum+=$3} END {print "Total Memory: " sum "Mi"}'

# ãƒªã‚½ãƒ¼ã‚¹è¦æ±‚/åˆ¶é™ã®é›†è¨ˆ
kubectl get pods --all-namespaces -o json | jq -r '
  .items[] |
  .spec.containers[] |
  select(.resources.requests != null) |
  {
    namespace: .metadata.namespace,
    pod: .metadata.name,
    container: .name,
    cpu_request: .resources.requests.cpu,
    memory_request: .resources.requests.memory,
    cpu_limit: .resources.limits.cpu,
    memory_limit: .resources.limits.memory
  }
' | jq -s 'group_by(.namespace) | map({namespace: .[0].namespace, pods: length})'
```

### 4.2 Growth Planningï¼ˆæˆé•·äºˆæ¸¬ï¼‰

```python
# growth_forecast.py - ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã®äºˆæ¸¬

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

# Prometheusã‹ã‚‰éå»90æ—¥é–“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—ï¼ˆç–‘ä¼¼ãƒ‡ãƒ¼ã‚¿ï¼‰
dates = pd.date_range('2023-10-15', '2024-01-15', freq='D')
# å®Ÿéš›ã¯Prometheusã‹ã‚‰ä»¥ä¸‹ã®ã‚¯ã‚¨ãƒªã§å–å¾—:
# sum(rate(container_cpu_usage_seconds_total[1h]))
# sum(container_memory_working_set_bytes)

cpu_usage = np.linspace(100, 250, len(dates)) + np.random.normal(0, 10, len(dates))
memory_usage = np.linspace(500, 1200, len(dates)) + np.random.normal(0, 50, len(dates))

df = pd.DataFrame({
    'date': dates,
    'cpu_cores': cpu_usage,
    'memory_gb': memory_usage
})

# ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«
X = np.arange(len(df)).reshape(-1, 1)
y_cpu = df['cpu_cores'].values
y_memory = df['memory_gb'].values

model_cpu = LinearRegression().fit(X, y_cpu)
model_memory = LinearRegression().fit(X, y_memory)

# ä»Šå¾Œ90æ—¥é–“ã®äºˆæ¸¬
future_days = 90
future_X = np.arange(len(df), len(df) + future_days).reshape(-1, 1)

predicted_cpu = model_cpu.predict(future_X)
predicted_memory = model_memory.predict(future_X)

# çµæœå‡ºåŠ›
print("=== Growth Forecast (90 days) ===")
print(f"Current CPU usage: {y_cpu[-1]:.1f} cores")
print(f"Predicted CPU usage: {predicted_cpu[-1]:.1f} cores")
print(f"Growth rate: {((predicted_cpu[-1] / y_cpu[-1]) - 1) * 100:.1f}%")
print()
print(f"Current Memory usage: {y_memory[-1]:.1f} GB")
print(f"Predicted Memory usage: {predicted_memory[-1]:.1f} GB")
print(f"Growth rate: {((predicted_memory[-1] / y_memory[-1]) - 1) * 100:.1f}%")

# å®¹é‡ä¸è¶³ã®è­¦å‘Š
cluster_cpu_capacity = 500  # cores
cluster_memory_capacity = 2000  # GB

if predicted_cpu[-1] > cluster_cpu_capacity * 0.8:
    print(f"\nâš ï¸  CPU capacity warning: Predicted usage will exceed 80% in {future_days} days")
    days_until_full = int((cluster_cpu_capacity * 0.8 - y_cpu[-1]) / (model_cpu.coef_[0]))
    print(f"    Estimated days until 80% capacity: {days_until_full}")

if predicted_memory[-1] > cluster_memory_capacity * 0.8:
    print(f"\nâš ï¸  Memory capacity warning: Predicted usage will exceed 80% in {future_days} days")
    days_until_full = int((cluster_memory_capacity * 0.8 - y_memory[-1]) / (model_memory.coef_[0]))
    print(f"    Estimated days until 80% capacity: {days_until_full}")
```

### 4.3 Bin Packing åŠ¹ç‡

```bash
# Bin packing åŠ¹ç‡ã®æ¸¬å®š
# ãƒ»ãƒãƒ¼ãƒ‰ã®ãƒªã‚½ãƒ¼ã‚¹ãŒã©ã‚Œã ã‘åŠ¹ç‡çš„ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‹

# å„ãƒãƒ¼ãƒ‰ã®ãƒªã‚½ãƒ¼ã‚¹å‰²å½“åŠ¹ç‡ã‚’è¨ˆç®—
kubectl get nodes -o json | jq -r '
  .items[] |
  {
    name: .metadata.name,
    allocatable_cpu: .status.allocatable.cpu,
    allocatable_memory: .status.allocatable.memory
  }
' > /tmp/nodes.json

kubectl get pods --all-namespaces -o json | jq -r '
  .items[] |
  select(.status.phase == "Running") |
  {
    node: .spec.nodeName,
    cpu_request: (.spec.containers[].resources.requests.cpu // "0"),
    memory_request: (.spec.containers[].resources.requests.memory // "0")
  }
' > /tmp/pods.json

# Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã§é›†è¨ˆ
cat > /tmp/binpacking.py << 'EOF'
import json
import re

def parse_cpu(cpu_str):
    """CPUæ–‡å­—åˆ—ï¼ˆ1000m, 1ãªã©ï¼‰ã‚’millicoresã«å¤‰æ›"""
    if cpu_str.endswith('m'):
        return int(cpu_str[:-1])
    else:
        return int(float(cpu_str) * 1000)

def parse_memory(mem_str):
    """ãƒ¡ãƒ¢ãƒªæ–‡å­—åˆ—ï¼ˆ1Gi, 1000Miãªã©ï¼‰ã‚’MiBã«å¤‰æ›"""
    if mem_str.endswith('Ki'):
        return int(mem_str[:-2]) / 1024
    elif mem_str.endswith('Mi'):
        return int(mem_str[:-2])
    elif mem_str.endswith('Gi'):
        return int(mem_str[:-2]) * 1024
    else:
        return int(mem_str) / 1024 / 1024

# ãƒãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
with open('/tmp/nodes.json') as f:
    nodes = {}
    for line in f:
        node = json.loads(line)
        nodes[node['name']] = {
            'cpu': parse_cpu(node['allocatable_cpu']),
            'memory': parse_memory(node['allocatable_memory']),
            'used_cpu': 0,
            'used_memory': 0
        }

# Podãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
with open('/tmp/pods.json') as f:
    for line in f:
        pod = json.loads(line)
        if pod['node'] in nodes:
            nodes[pod['node']]['used_cpu'] += parse_cpu(pod['cpu_request'])
            nodes[pod['node']]['used_memory'] += parse_memory(pod['memory_request'])

# åŠ¹ç‡è¨ˆç®—
print("Node Bin Packing Efficiency:")
print("-" * 80)
print(f"{'Node':<20} {'CPU %':<10} {'Memory %':<10} {'Efficiency':<15}")
print("-" * 80)

total_cpu = 0
total_used_cpu = 0
total_memory = 0
total_used_memory = 0

for node_name, data in nodes.items():
    cpu_pct = (data['used_cpu'] / data['cpu']) * 100
    mem_pct = (data['used_memory'] / data['memory']) * 100
    efficiency = min(cpu_pct, mem_pct)  # åˆ¶ç´„ãƒªã‚½ãƒ¼ã‚¹ã®åŠ¹ç‡

    print(f"{node_name:<20} {cpu_pct:>8.1f}% {mem_pct:>9.1f}% {efficiency:>13.1f}%")

    total_cpu += data['cpu']
    total_used_cpu += data['used_cpu']
    total_memory += data['memory']
    total_used_memory += data['used_memory']

print("-" * 80)
cluster_cpu_pct = (total_used_cpu / total_cpu) * 100
cluster_mem_pct = (total_used_memory / total_memory) * 100
cluster_efficiency = min(cluster_cpu_pct, cluster_mem_pct)

print(f"{'CLUSTER TOTAL':<20} {cluster_cpu_pct:>8.1f}% {cluster_mem_pct:>9.1f}% {cluster_efficiency:>13.1f}%")
print()
print("Interpretation:")
print("- High efficiency (>70%): Good resource utilization")
print("- Medium efficiency (40-70%): Acceptable, room for improvement")
print("- Low efficiency (<40%): Poor resource utilization, consider:")
print("  1. Adjusting Pod requests/limits")
print("  2. Using smaller instance types")
print("  3. Consolidating workloads")
EOF

python3 /tmp/binpacking.py
```

---

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é‹ç”¨ã®ä¸»è¦éƒ¨åˆ†ã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã¾ã™ã€‚æœ€å¾Œã«ã€Œãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã€ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆã—ã¦Phase 1ã‚’å®Œæˆã•ã›ã¾ã™ã€‚

## ğŸ“š å‚è€ƒãƒªã‚½ãƒ¼ã‚¹

- [Kubernetes Upgrade Best Practices](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/)
- [Node Problem Detector](https://github.com/kubernetes/node-problem-detector)
- [Certificate Management](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/)
- [Capacity Planning](https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/)
