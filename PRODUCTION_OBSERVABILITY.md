# Kubernetesæœ¬ç•ªç’°å¢ƒ å¯è¦³æ¸¬æ€§ï¼ˆObservabilityï¼‰å®Œå…¨ã‚¬ã‚¤ãƒ‰

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€Kubernetesæœ¬ç•ªç’°å¢ƒã§å¿…é ˆã¨ãªã‚‹å¯è¦³æ¸¬æ€§ã®å®Ÿè£…ã‚’ã€æŠ€è¡“çš„åŸç†ã‹ã‚‰å®Ÿè·µã¾ã§å¾¹åº•çš„ã«è§£èª¬ã—ã¾ã™ã€‚

## ğŸ“š ç›®æ¬¡

1. [ãƒ­ã‚°ç®¡ç†](#1-ãƒ­ã‚°ç®¡ç†)
2. [ãƒ¡ãƒˆãƒªã‚¯ã‚¹](#2-ãƒ¡ãƒˆãƒªã‚¯ã‚¹)
3. [åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°](#3-åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°)
4. [ã‚¢ãƒ©ãƒ¼ãƒˆè¨­è¨ˆ](#4-ã‚¢ãƒ©ãƒ¼ãƒˆè¨­è¨ˆ)
5. [ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰](#5-ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰)

---

## 1. ãƒ­ã‚°ç®¡ç†

### 1.1 ãƒ­ã‚°é›†ç´„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¯”è¼ƒ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Option 1: EFK Stack (Elasticsearch + Fluent Bit + Kibana) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Kubernetes Cluster
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Node 1                        Node 2                    â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚ â”‚ Pod Logs â”‚                 â”‚ Pod Logs â”‚              â”‚
â”‚ â”‚ stdout   â”‚                 â”‚ stdout   â”‚              â”‚
â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜              â”‚
â”‚      â”‚ JSON logs                  â”‚                    â”‚
â”‚      â–¼                            â–¼                    â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚ â”‚ Fluent Bit      â”‚         â”‚ Fluent Bit      â”‚        â”‚
â”‚ â”‚ (DaemonSet)     â”‚         â”‚ (DaemonSet)     â”‚        â”‚
â”‚ â”‚ ãƒ»ãƒ­ã‚°åé›†      â”‚         â”‚ ãƒ»ãƒ‘ãƒ¼ã‚¹        â”‚        â”‚
â”‚ â”‚ ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°â”‚         â”‚ ãƒ»ã‚¨ãƒ³ãƒªãƒƒãƒ    â”‚        â”‚
â”‚ â”‚ ãƒ»è»½é‡(~15MB)   â”‚         â”‚                 â”‚        â”‚
â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚      â”‚                           â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                           â”‚
       â”‚ Forward                   â”‚
       â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Elasticsearch Cluster                       â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ Master    â”‚ â”‚ Data      â”‚ â”‚ Data      â”‚  â”‚
â”‚ â”‚ Node      â”‚ â”‚ Node      â”‚ â”‚ Node      â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚ ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ                          â”‚
â”‚ ãƒ»å…¨æ–‡æ¤œç´¢                                  â”‚
â”‚ ãƒ»ã‚¢ã‚°ãƒªã‚²ãƒ¼ã‚·ãƒ§ãƒ³                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ Query
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kibana                                      â”‚
â”‚ ãƒ»ãƒ­ã‚°æ¤œç´¢UI                                â”‚
â”‚ ãƒ»ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰                            â”‚
â”‚ ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆ                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Option 2: Loki Stack (Loki + Promtail + Grafana)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Kubernetes Cluster
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Node 1                        Node 2                    â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚ â”‚ Pod Logs â”‚                 â”‚ Pod Logs â”‚              â”‚
â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜              â”‚
â”‚      â”‚                            â”‚                    â”‚
â”‚      â–¼                            â–¼                    â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚ â”‚ Promtail        â”‚         â”‚ Promtail        â”‚        â”‚
â”‚ â”‚ (DaemonSet)     â”‚         â”‚ (DaemonSet)     â”‚        â”‚
â”‚ â”‚ ãƒ»ãƒ©ãƒ™ãƒ«æŠ½å‡º    â”‚         â”‚ ãƒ»ãƒ­ã‚°è¡Œã®ã¿é€ä¿¡â”‚        â”‚
â”‚ â”‚ ãƒ»è»½é‡          â”‚         â”‚ (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãªã—)       â”‚
â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚      â”‚                           â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Push (gRPC)               â”‚
       â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Loki (StatefulSet)                          â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚ â”‚ Distributor â†’ Ingester â†’ Querier    â”‚    â”‚
â”‚ â”‚ ãƒ»ãƒ­ã‚°ã‚’åœ§ç¸®ä¿å­˜                     â”‚    â”‚
â”‚ â”‚ ãƒ»ãƒ©ãƒ™ãƒ«ã®ã¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹             â”‚    â”‚
â”‚ â”‚ ãƒ»ãƒ­ã‚°æœ¬æ–‡ã¯åœ§ç¸®ã—ã¦S3/GCSç­‰ã«ä¿å­˜  â”‚    â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚ ã‚³ã‚¹ãƒˆ: Elasticsearchã®1/10ä»¥ä¸‹             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ LogQL Query
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Grafana                                     â”‚
â”‚ ãƒ»Lokiãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹çµ±åˆ                      â”‚
â”‚ ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨ãƒ­ã‚°ã®ç›¸é–¢                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### EFK vs Loki æ¯”è¼ƒè¡¨

| è¦³ç‚¹ | EFK (Elasticsearch) | Loki |
|------|---------------------|------|
| **ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹** | å…¨æ–‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ | ãƒ©ãƒ™ãƒ«ã®ã¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ |
| **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚³ã‚¹ãƒˆ** | é«˜ã„ | ä½ã„ï¼ˆ1/10ç¨‹åº¦ï¼‰ |
| **æ¤œç´¢é€Ÿåº¦** | é«˜é€Ÿï¼ˆå…¨æ–‡æ¤œç´¢ï¼‰ | ãƒ©ãƒ™ãƒ«æ¤œç´¢ã¯é«˜é€Ÿã€å…¨æ–‡æ¤œç´¢ã¯é…ã„ |
| **ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡** | å¤§ï¼ˆç‰¹ã«ãƒ¡ãƒ¢ãƒªï¼‰ | å° |
| **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£** | æ°´å¹³ã‚¹ã‚±ãƒ¼ãƒ«å¯èƒ½ | æ°´å¹³ã‚¹ã‚±ãƒ¼ãƒ«å¯èƒ½ |
| **ä¿æŒæœŸé–“** | ã‚³ã‚¹ãƒˆçš„ã«çŸ­ã‚(30-90æ—¥) | é•·æœŸå¯èƒ½(1å¹´+) |
| **å­¦ç¿’æ›²ç·š** | æ€¥ï¼ˆLucene Queryï¼‰ | ç·©ã‚„ã‹ï¼ˆLogQL â‰’ PromQLï¼‰ |
| **é©ç”¨ã‚±ãƒ¼ã‚¹** | è¤‡é›‘ãªæ¤œç´¢ã‚¯ã‚¨ãƒªãŒå¿…è¦ | ã‚³ã‚¹ãƒˆé‡è¦–ã€Prometheusä½µç”¨ |

### 1.2 Fluent Bit ã®è©³ç´°å®Ÿè£…

#### Fluent Bit ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
Fluent Bit ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   INPUT     â”‚ â†’  â”‚   PARSER     â”‚ â†’  â”‚   FILTER    â”‚
â”‚ (ãƒ­ã‚°åé›†)  â”‚    â”‚ (ãƒ‘ãƒ¼ã‚¹)     â”‚    â”‚ (å¤‰æ›/è¿½åŠ )  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“                                        â†“
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   BUFFER    â”‚
                  â”‚ (ãƒ¡ãƒ¢ãƒª/FS)  â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   OUTPUT    â”‚
                  â”‚ (é€ä¿¡å…ˆ)     â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Fluent Bit ConfigMap

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
  namespace: logging
data:
  fluent-bit.conf: |
    [SERVICE]
        Flush         5
        Daemon        off
        Log_Level     info
        Parsers_File  parsers.conf

    # INPUT: Kubernetesã‚³ãƒ³ãƒ†ãƒŠãƒ­ã‚°ã®åé›†
    [INPUT]
        Name              tail
        Path              /var/log/containers/*.log
        Parser            cri  # containerd/CRI-Oå½¢å¼
        Tag               kube.*
        Refresh_Interval  5
        Mem_Buf_Limit     5MB
        Skip_Long_Lines   On
        DB                /var/log/flb-kube.db  # ãƒã‚¸ã‚·ãƒ§ãƒ³è¨˜éŒ²

    # INPUT: systemd journalï¼ˆkubelet, dockerãƒ­ã‚°ç­‰ï¼‰
    [INPUT]
        Name            systemd
        Tag             systemd.*
        Read_From_Tail  On
        Strip_Underscores On

    # FILTER: Kubernetes ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ 
    [FILTER]
        Name                kubernetes
        Match               kube.*
        Kube_URL            https://kubernetes.default.svc:443
        Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token
        Kube_Tag_Prefix     kube.var.log.containers.
        Merge_Log           On    # JSONãƒ­ã‚°ã‚’ãƒ‘ãƒ¼ã‚¹
        Keep_Log            Off   # å…ƒã®log fieldã¯å‰Šé™¤
        K8S-Logging.Parser  On
        K8S-Logging.Exclude On
        Labels              On    # Pod labels ã‚’è¿½åŠ 
        Annotations         Off   # Annotations ã¯é™¤å¤–ï¼ˆãƒ‡ãƒ¼ã‚¿é‡å‰Šæ¸›ï¼‰

    # FILTER: ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®æŠ½å‡ºï¼ˆæ§‹é€ åŒ–ãƒ­ã‚°ï¼‰
    [FILTER]
        Name    parser
        Match   kube.*
        Key_Name log
        Parser   json-log
        Reserve_Data On  # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã‚‚å…ƒãƒ‡ãƒ¼ã‚¿ä¿æŒ

    # FILTER: ä¸è¦ãªãƒ­ã‚°ã®é™¤å¤–
    [FILTER]
        Name    grep
        Match   kube.*
        Exclude kubernetes_namespace_name kube-system
        Exclude log healthcheck

    # FILTER: ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã®å¤‰æ›´
    [FILTER]
        Name       modify
        Match      kube.*
        Rename     log message
        Add        cluster_name production-eks
        Remove     stream

    # OUTPUT: Elasticsearch
    [OUTPUT]
        Name            es
        Match           kube.*
        Host            elasticsearch.logging.svc
        Port            9200
        Index           kubernetes
        Type            _doc
        Logstash_Format On
        Logstash_Prefix kubernetes
        Logstash_DateFormat %Y.%m.%d
        Retry_Limit     5
        Buffer_Size     False  # ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«é€ä¿¡
        Trace_Error     On

    # OUTPUT: Loki (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
    [OUTPUT]
        Name        loki
        Match       kube.*
        Host        loki.logging.svc
        Port        3100
        Labels      job=fluentbit, cluster=production
        Label_keys  $kubernetes['namespace_name'],$kubernetes['pod_name'],$kubernetes['container_name']

  parsers.conf: |
    # CRIå½¢å¼ã®ãƒ‘ãƒ¼ã‚µãƒ¼ï¼ˆcontainerd/CRI-Oï¼‰
    [PARSER]
        Name        cri
        Format      regex
        Regex       ^(?<time>[^ ]+) (?<stream>stdout|stderr) (?<logtag>[^ ]*) (?<log>.*)$
        Time_Key    time
        Time_Format %Y-%m-%dT%H:%M:%S.%L%z

    # JSONå½¢å¼ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°
    [PARSER]
        Name        json-log
        Format      json
        Time_Key    timestamp
        Time_Format %Y-%m-%dT%H:%M:%S.%L%z
        Time_Keep   On

    # Nginx ã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°
    [PARSER]
        Name        nginx
        Format      regex
        Regex       ^(?<remote>[^ ]*) (?<host>[^ ]*) (?<user>[^ ]*) \[(?<time>[^\]]*)\] "(?<method>\S+)(?: +(?<path>[^\"]*?)(?: +\S*)?)?" (?<code>[^ ]*) (?<size>[^ ]*)(?: "(?<referer>[^\"]*)" "(?<agent>[^\"]*)")
        Time_Key    time
        Time_Format %d/%b/%Y:%H:%M:%S %z
```

#### Fluent Bit DaemonSet

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluent-bit
  namespace: logging
  labels:
    app: fluent-bit
spec:
  selector:
    matchLabels:
      app: fluent-bit
  template:
    metadata:
      labels:
        app: fluent-bit
    spec:
      serviceAccountName: fluent-bit
      tolerations:
      # ãƒã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰ã«ã‚‚é…ç½®
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule

      containers:
      - name: fluent-bit
        image: fluent/fluent-bit:2.1
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 256Mi

        volumeMounts:
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿å–ã‚Š
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
        - name: fluent-bit-config
          mountPath: /fluent-bit/etc/
        # ãƒã‚¸ã‚·ãƒ§ãƒ³DBï¼ˆå†èµ·å‹•æ™‚ã®é‡è¤‡é˜²æ­¢ï¼‰
        - name: position-db
          mountPath: /var/log

      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: fluent-bit-config
        configMap:
          name: fluent-bit-config
      - name: position-db
        emptyDir: {}

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluent-bit
  namespace: logging

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluent-bit
rules:
- apiGroups: [""]
  resources:
  - namespaces
  - pods
  - pods/logs
  verbs:
  - get
  - list
  - watch

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fluent-bit
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: fluent-bit
subjects:
- kind: ServiceAccount
  name: fluent-bit
  namespace: logging
```

### 1.3 æ§‹é€ åŒ–ãƒ­ã‚°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

#### ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã®æ§‹é€ åŒ–ãƒ­ã‚°å®Ÿè£…

```go
// Go ã§ã®æ§‹é€ åŒ–ãƒ­ã‚°ä¾‹ï¼ˆzapï¼‰
package main

import (
    "go.uber.org/zap"
    "go.uber.org/zap/zapcore"
)

func main() {
    // Productionç”¨ã®è¨­å®š
    config := zap.NewProductionConfig()
    config.EncoderConfig.TimeKey = "timestamp"
    config.EncoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder

    logger, _ := config.Build()
    defer logger.Sync()

    // æ§‹é€ åŒ–ãƒ­ã‚°å‡ºåŠ›
    logger.Info("User login",
        zap.String("user_id", "12345"),
        zap.String("ip_address", "192.168.1.100"),
        zap.Duration("response_time", 123*time.Millisecond),
        zap.Int("status_code", 200),
    )

    // ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
    logger.Error("Database connection failed",
        zap.String("database", "postgres"),
        zap.String("host", "db.example.com"),
        zap.Error(err),
        zap.Stack("stacktrace"),  # ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’å«ã‚ã‚‹
    )
}

// å‡ºåŠ›JSON:
// {
//   "level": "info",
//   "timestamp": "2024-01-15T10:30:00.123Z",
//   "msg": "User login",
//   "user_id": "12345",
//   "ip_address": "192.168.1.100",
//   "response_time": 0.123,
//   "status_code": 200
// }
```

```python
# Python ã§ã®æ§‹é€ åŒ–ãƒ­ã‚°ä¾‹ï¼ˆstructlogï¼‰
import structlog

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    wrapper_class=structlog.stdlib.BoundLogger,
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)

log = structlog.get_logger()

# æ§‹é€ åŒ–ãƒ­ã‚°å‡ºåŠ›
log.info("user_login",
         user_id="12345",
         ip_address="192.168.1.100",
         response_time=0.123,
         status_code=200)

# ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
try:
    connect_to_database()
except Exception as e:
    log.error("database_connection_failed",
              database="postgres",
              host="db.example.com",
              exc_info=True)  # ä¾‹å¤–æƒ…å ±ã‚’å«ã‚ã‚‹
```

#### æ¨™æº–åŒ–ã•ã‚ŒãŸãƒ­ã‚°ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰

```json
{
  // å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
  "timestamp": "2024-01-15T10:30:00.123Z",
  "level": "info",                          // debug, info, warn, error, fatal
  "message": "User authentication successful",

  // ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è­˜åˆ¥
  "service": "auth-service",
  "version": "1.2.3",
  "environment": "production",

  // Kubernetes ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆFluent BitãŒè‡ªå‹•è¿½åŠ ï¼‰
  "kubernetes": {
    "namespace_name": "prod",
    "pod_name": "auth-service-7d9f8b-xyz",
    "container_name": "auth",
    "labels": {
      "app": "auth-service",
      "version": "v1.2.3"
    }
  },

  // ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
  "request": {
    "id": "req-12345",                      // Trace ID
    "method": "POST",
    "path": "/api/v1/login",
    "ip": "192.168.1.100",
    "user_agent": "Mozilla/5.0..."
  },

  // ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
  "user": {
    "id": "user-67890",
    "email": "user@example.com"
  },

  // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
  "performance": {
    "duration_ms": 123,
    "db_query_ms": 45,
    "cache_hit": true
  },

  // ã‚¨ãƒ©ãƒ¼æƒ…å ±ï¼ˆã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®å ´åˆï¼‰
  "error": {
    "type": "DatabaseConnectionError",
    "message": "Connection timeout",
    "stack_trace": "...",
    "cause": "Network unreachable"
  }
}
```

---

## 2. ãƒ¡ãƒˆãƒªã‚¯ã‚¹

### 2.1 Prometheus Operator ã®è©³ç´°

#### Prometheus Operator ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kubernetes Cluster                                     â”‚
â”‚                                                         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚ â”‚ Custom Resource Definitions (CRDs)             â”‚     â”‚
â”‚ â”‚ â”œâ”€ Prometheus                                  â”‚     â”‚
â”‚ â”‚ â”œâ”€ ServiceMonitor                              â”‚     â”‚
â”‚ â”‚ â”œâ”€ PodMonitor                                  â”‚     â”‚
â”‚ â”‚ â”œâ”€ PrometheusRule                              â”‚     â”‚
â”‚ â”‚ â””â”€ Alertmanager                                â”‚     â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                      â”‚                                  â”‚
â”‚                      â”‚ watch                            â”‚
â”‚                      â–¼                                  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚ â”‚ Prometheus Operator (Deployment)               â”‚     â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚
â”‚ â”‚ â”‚ Controller Loop:                           â”‚ â”‚     â”‚
â”‚ â”‚ â”‚ 1. Watch CRs                               â”‚ â”‚     â”‚
â”‚ â”‚ â”‚ 2. Generate Prometheus config              â”‚ â”‚     â”‚
â”‚ â”‚ â”‚ 3. Create/Update StatefulSet               â”‚ â”‚     â”‚
â”‚ â”‚ â”‚ 4. Mount ConfigMap/Secrets                 â”‚ â”‚     â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                      â”‚                                  â”‚
â”‚                      â”‚ creates/updates                  â”‚
â”‚                      â–¼                                  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚ â”‚ Prometheus StatefulSet                         â”‚     â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚     â”‚
â”‚ â”‚ â”‚ prometheus-0                             â”‚   â”‚     â”‚
â”‚ â”‚ â”‚ â”œâ”€ Config from operator                  â”‚   â”‚     â”‚
â”‚ â”‚ â”‚ â”œâ”€ PersistentVolume (TSDB)               â”‚   â”‚     â”‚
â”‚ â”‚ â”‚ â””â”€ Scrape targets from ServiceMonitors   â”‚   â”‚     â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚     â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                      â”‚                                  â”‚
â”‚                      â”‚ scrape (pull)                    â”‚
â”‚                      â–¼                                  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚ â”‚ Application Pods                               â”‚     â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚     â”‚
â”‚ â”‚ â”‚ myapp-1          â”‚  â”‚ myapp-2          â”‚    â”‚     â”‚
â”‚ â”‚ â”‚ :8080/metrics    â”‚  â”‚ :8080/metrics    â”‚    â”‚     â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚     â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Prometheus CRï¼ˆCustom Resourceï¼‰ã®å®šç¾©

```yaml
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: main
  namespace: monitoring
spec:
  # ãƒ¬ãƒ—ãƒªã‚«æ•°ï¼ˆHAæ§‹æˆï¼‰
  replicas: 2

  # ãƒ‡ãƒ¼ã‚¿ä¿æŒæœŸé–“
  retention: 30d
  retentionSize: "100GB"

  # ãƒªã‚½ãƒ¼ã‚¹è¨­å®š
  resources:
    requests:
      memory: 2Gi
      cpu: 1000m
    limits:
      memory: 4Gi
      cpu: 2000m

  # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
  storage:
    volumeClaimTemplate:
      spec:
        storageClassName: fast-ssd
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 100Gi

  # ServiceMonitor ã®é¸æŠ
  serviceMonitorSelector:
    matchLabels:
      prometheus: main

  # PodMonitor ã®é¸æŠ
  podMonitorSelector:
    matchLabels:
      prometheus: main

  # PrometheusRule ã®é¸æŠ
  ruleSelector:
    matchLabels:
      prometheus: main

  # Alertmanager é€£æº
  alerting:
    alertmanagers:
    - namespace: monitoring
      name: main
      port: web

  # å¤–éƒ¨ãƒ©ãƒ™ãƒ«ï¼ˆThanosé€£æºç­‰ã§ä½¿ç”¨ï¼‰
  externalLabels:
    cluster: production-eks
    region: ap-northeast-1

  # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 2000

  # Service Account
  serviceAccountName: prometheus
```

### 2.2 ServiceMonitor ã¨ PodMonitor

#### ServiceMonitor: ServiceçµŒç”±ã§ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—

```yaml
# 1. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®Service
---
apiVersion: v1
kind: Service
metadata:
  name: myapp
  namespace: prod
  labels:
    app: myapp
spec:
  selector:
    app: myapp
  ports:
  - name: web
    port: 8080
  - name: metrics  # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    port: 9090

---
# 2. ServiceMonitor ã®å®šç¾©
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: myapp-monitor
  namespace: prod
  labels:
    prometheus: main  # Prometheus CRã®selectorã«ãƒãƒƒãƒ
spec:
  # ç›£è¦–å¯¾è±¡Serviceã®é¸æŠ
  selector:
    matchLabels:
      app: myapp

  # Namespace ã®é¸æŠï¼ˆçœç•¥æ™‚ã¯åŒä¸€namespaceï¼‰
  namespaceSelector:
    matchNames:
    - prod
    - staging

  # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—è¨­å®š
  endpoints:
  - port: metrics        # Serviceã®portå
    interval: 30s        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—é–“éš”
    path: /metrics       # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‘ã‚¹
    scheme: http

    # TLSè¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    # tlsConfig:
    #   insecureSkipVerify: false
    #   ca:
    #     secret:
    #       name: myapp-tls
    #       key: ca.crt

    # Basicèªè¨¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    # basicAuth:
    #   username:
    #     name: myapp-metrics-auth
    #     key: username
    #   password:
    #     name: myapp-metrics-auth
    #     key: password

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å†ãƒ©ãƒ™ãƒªãƒ³ã‚°
    relabelings:
    # ãƒ©ãƒ™ãƒ«ã®è¿½åŠ 
    - sourceLabels: [__meta_kubernetes_pod_node_name]
      targetLabel: node

    # ä¸è¦ãªãƒ©ãƒ™ãƒ«ã®å‰Šé™¤
    - action: labeldrop
      regex: __meta_kubernetes_pod_label_pod_template_hash

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å€¤ã®å¤‰æ›
    metricRelabelings:
    # ç‰¹å®šãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®é™¤å¤–
    - sourceLabels: [__name__]
      regex: 'go_.*'       # Go runtime ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’é™¤å¤–
      action: drop
```

#### PodMonitor: Podç›´æ¥ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—

```yaml
# PodMonitor: Serviceã‚’çµŒç”±ã›ãšPodã‹ã‚‰ç›´æ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: myapp-pod-monitor
  namespace: prod
  labels:
    prometheus: main
spec:
  # ç›£è¦–å¯¾è±¡Podã®é¸æŠ
  selector:
    matchLabels:
      app: myapp

  # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—è¨­å®š
  podMetricsEndpoints:
  - port: metrics        # Pod ã® container port å
    interval: 15s
    path: /metrics

  # Namespace ã®é¸æŠ
  namespaceSelector:
    matchNames:
    - prod

# PodMonitor vs ServiceMonitor ã®ä½¿ã„åˆ†ã‘:
# ServiceMonitor:
#   ãƒ»æ¨™æº–çš„ãªç”¨é€”
#   ãƒ»Serviceã‚’çµŒç”±ã™ã‚‹ãŸã‚ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚¹ã•ã‚Œã‚‹
#   ãƒ»ServiceãŒå­˜åœ¨ã™ã‚‹å ´åˆ
#
# PodMonitor:
#   ãƒ»StatefulSetç­‰ã€Podå€‹åˆ¥ã®ç›£è¦–ãŒå¿…è¦ãªå ´åˆ
#   ãƒ»Headless Serviceã®å ´åˆ
#   ãƒ»DaemonSetï¼ˆå„Nodeã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å€‹åˆ¥å–å¾—ï¼‰
```

### 2.3 Recording Rules ã¨ Alerting Rules

#### Recording Rules: äº‹å‰é›†è¨ˆ

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: recording-rules
  namespace: monitoring
  labels:
    prometheus: main
spec:
  groups:
  # ã‚°ãƒ«ãƒ¼ãƒ—1: HTTP ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®é›†è¨ˆ
  - name: http_metrics
    interval: 30s  # è©•ä¾¡é–“éš”
    rules:
    # 1. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã”ã¨ã®QPS
    - record: job:http_requests_total:rate5m
      expr: |
        sum(rate(http_requests_total[5m])) by (job)

    # 2. ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã”ã¨ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼ˆ99ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ï¼‰
    - record: job:http_request_duration_seconds:p99
      expr: |
        histogram_quantile(0.99,
          sum(rate(http_request_duration_seconds_bucket[5m])) by (job, le)
        )

    # 3. ã‚¨ãƒ©ãƒ¼ç‡
    - record: job:http_requests:error_rate5m
      expr: |
        sum(rate(http_requests_total{status=~"5.."}[5m])) by (job)
        /
        sum(rate(http_requests_total[5m])) by (job)

  # ã‚°ãƒ«ãƒ¼ãƒ—2: ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡
  - name: resource_usage
    interval: 1m
    rules:
    # CPUä½¿ç”¨ç‡ï¼ˆPodã”ã¨ï¼‰
    - record: namespace_pod:container_cpu_usage:sum
      expr: |
        sum(rate(container_cpu_usage_seconds_total{container!=""}[5m]))
        by (namespace, pod)

    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
    - record: namespace_pod:container_memory_usage:sum
      expr: |
        sum(container_memory_working_set_bytes{container!=""})
        by (namespace, pod)

    # ãƒ‡ã‚£ã‚¹ã‚¯I/O
    - record: node:disk_io_time_seconds:rate5m
      expr: |
        rate(node_disk_io_time_seconds_total[5m])
```

#### Alerting Rules: ã‚¢ãƒ©ãƒ¼ãƒˆå®šç¾©

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: alerting-rules
  namespace: monitoring
  labels:
    prometheus: main
spec:
  groups:
  # ã‚°ãƒ«ãƒ¼ãƒ—1: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¢ãƒ©ãƒ¼ãƒˆ
  - name: application_alerts
    interval: 30s
    rules:
    # é«˜ã‚¨ãƒ©ãƒ¼ç‡
    - alert: HighErrorRate
      expr: |
        job:http_requests:error_rate5m > 0.05
      for: 5m  # 5åˆ†é–“ç¶™ç¶šã—ãŸã‚‰ã‚¢ãƒ©ãƒ¼ãƒˆ
      labels:
        severity: warning
        team: backend
      annotations:
        summary: "High error rate detected"
        description: |
          Error rate is {{ $value | humanizePercentage }} for {{ $labels.job }}
          (threshold: 5%)

    # é«˜ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·
    - alert: HighLatency
      expr: |
        job:http_request_duration_seconds:p99 > 1.0
      for: 10m
      labels:
        severity: warning
        team: backend
      annotations:
        summary: "High latency detected"
        description: |
          99th percentile latency is {{ $value }}s for {{ $labels.job }}
          (threshold: 1s)

    # ã‚µãƒ¼ãƒ“ã‚¹ãƒ€ã‚¦ãƒ³
    - alert: ServiceDown
      expr: |
        up{job="myapp"} == 0
      for: 1m
      labels:
        severity: critical
        team: backend
      annotations:
        summary: "Service is down"
        description: |
          {{ $labels.job }} on {{ $labels.instance }} is down

  # ã‚°ãƒ«ãƒ¼ãƒ—2: ã‚¤ãƒ³ãƒ•ãƒ©ã‚¢ãƒ©ãƒ¼ãƒˆ
  - name: infrastructure_alerts
    interval: 1m
    rules:
    # ãƒãƒ¼ãƒ‰ãƒ€ã‚¦ãƒ³
    - alert: NodeDown
      expr: |
        up{job="node-exporter"} == 0
      for: 5m
      labels:
        severity: critical
        team: infrastructure
      annotations:
        summary: "Node is down"
        description: "Node {{ $labels.instance }} is down"

    # é«˜CPUä½¿ç”¨ç‡
    - alert: HighCPUUsage
      expr: |
        (1 - avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) by (instance)) > 0.8
      for: 10m
      labels:
        severity: warning
        team: infrastructure
      annotations:
        summary: "High CPU usage"
        description: |
          CPU usage is {{ $value | humanizePercentage }} on {{ $labels.instance }}
          (threshold: 80%)

    # é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
    - alert: HighMemoryUsage
      expr: |
        (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) > 0.9
      for: 10m
      labels:
        severity: warning
        team: infrastructure
      annotations:
        summary: "High memory usage"
        description: |
          Memory usage is {{ $value | humanizePercentage }} on {{ $labels.instance }}
          (threshold: 90%)

    # ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³
    - alert: DiskSpaceLow
      expr: |
        (1 - (node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.*"} / node_filesystem_size_bytes)) > 0.85
      for: 10m
      labels:
        severity: warning
        team: infrastructure
      annotations:
        summary: "Disk space is low"
        description: |
          Disk usage is {{ $value | humanizePercentage }} on {{ $labels.instance }}:{{ $labels.mountpoint }}
          (threshold: 85%)

  # ã‚°ãƒ«ãƒ¼ãƒ—3: Kubernetesã‚¢ãƒ©ãƒ¼ãƒˆ
  - name: kubernetes_alerts
    interval: 1m
    rules:
    # Podå†èµ·å‹•ãŒé »ç¹
    - alert: PodCrashLooping
      expr: |
        rate(kube_pod_container_status_restarts_total[15m]) > 0
      for: 5m
      labels:
        severity: warning
        team: platform
      annotations:
        summary: "Pod is crash looping"
        description: |
          Pod {{ $labels.namespace }}/{{ $labels.pod }} is restarting frequently

    # PodãŒèµ·å‹•ã—ãªã„
    - alert: PodNotReady
      expr: |
        kube_pod_status_phase{phase!~"Running|Succeeded"} > 0
      for: 15m
      labels:
        severity: warning
        team: platform
      annotations:
        summary: "Pod is not ready"
        description: |
          Pod {{ $labels.namespace }}/{{ $labels.pod }} is in {{ $labels.phase }} phase

    # HPA ãŒä¸Šé™ã«é”ã—ã¦ã„ã‚‹
    - alert: HPAMaxedOut
      expr: |
        kube_horizontalpodautoscaler_status_current_replicas
        >=
        kube_horizontalpodautoscaler_spec_max_replicas
      for: 15m
      labels:
        severity: warning
        team: platform
      annotations:
        summary: "HPA has reached maximum replicas"
        description: |
          HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler }} is at maximum capacity
```

### 2.4 Alertmanager ã®è¨­å®š

```yaml
apiVersion: monitoring.coreos.com/v1alpha1
kind: AlertmanagerConfig
metadata:
  name: main-config
  namespace: monitoring
spec:
  # ãƒ«ãƒ¼ãƒˆè¨­å®š
  route:
    receiver: default
    groupBy: ['alertname', 'cluster', 'service']
    groupWait: 30s         # åŒã˜ã‚°ãƒ«ãƒ¼ãƒ—ã®ã‚¢ãƒ©ãƒ¼ãƒˆã‚’30ç§’å¾…ã£ã¦ã¾ã¨ã‚ã‚‹
    groupInterval: 5m      # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã•ã‚ŒãŸã‚¢ãƒ©ãƒ¼ãƒˆã®å†é€é–“éš”
    repeatInterval: 12h    # åŒã˜ã‚¢ãƒ©ãƒ¼ãƒˆã®å†é€é–“éš”

    routes:
    # Critical ã‚¢ãƒ©ãƒ¼ãƒˆã¯å³åº§ã«PagerDutyã¸
    - match:
        severity: critical
      receiver: pagerduty
      groupWait: 10s
      repeatInterval: 1h

    # Warningã‚¢ãƒ©ãƒ¼ãƒˆã¯slackã¸
    - match:
        severity: warning
      receiver: slack-warnings

    # ãƒãƒ¼ãƒ ã”ã¨ã®ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
    - match:
        team: backend
      receiver: slack-backend

    - match:
        team: infrastructure
      receiver: slack-infrastructure

  # Receiverå®šç¾©
  receivers:
  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆã™ã¹ã¦ã®ã‚¢ãƒ©ãƒ¼ãƒˆï¼‰
  - name: default
    slackConfigs:
    - apiURL:
        name: slack-webhook
        key: url
      channel: '#alerts-all'
      title: '{{ .GroupLabels.alertname }}'
      text: |
        {{ range .Alerts }}
        *Alert:* {{ .Labels.alertname }}
        *Severity:* {{ .Labels.severity }}
        *Description:* {{ .Annotations.description }}
        {{ end }}

  # PagerDutyï¼ˆCriticalç”¨ï¼‰
  - name: pagerduty
    pagerdutyConfigs:
    - routingKey:
        name: pagerduty-key
        key: routing-key
      description: '{{ .GroupLabels.alertname }}'
      severity: '{{ .CommonLabels.severity }}'

  # Slack - Warnings
  - name: slack-warnings
    slackConfigs:
    - apiURL:
        name: slack-webhook
        key: url
      channel: '#alerts-warnings'
      color: 'warning'

  # Slack - Backend team
  - name: slack-backend
    slackConfigs:
    - apiURL:
        name: slack-webhook
        key: url
      channel: '#team-backend-alerts'

  # Email
  - name: email
    emailConfigs:
    - to: 'ops-team@example.com'
      from: 'alertmanager@example.com'
      smarthost: 'smtp.gmail.com:587'
      authUsername: 'alertmanager@example.com'
      authPassword:
        name: smtp-password
        key: password
      headers:
        Subject: '[{{ .Status }}] {{ .GroupLabels.alertname }}'

  # æŠ‘åˆ¶ãƒ«ãƒ¼ãƒ«ï¼ˆInhibitionï¼‰
  inhibitRules:
  # NodeDownã®æ™‚ã¯ã€ãã®Nodeä¸Šã®Podã‚¢ãƒ©ãƒ¼ãƒˆã‚’æŠ‘åˆ¶
  - sourceMatch:
      alertname: NodeDown
    targetMatch:
      alertname: PodNotReady
    equal: ['instance']
```

---

## 3. åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°

### 3.1 OpenTelemetry ã®è©³ç´°

#### OpenTelemetry ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆGoä¾‹ï¼‰                                   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ import "go.opentelemetry.io/otel"                     â”‚ â”‚
â”‚ â”‚                                                       â”‚ â”‚
â”‚ â”‚ tracer := otel.Tracer("myapp")                        â”‚ â”‚
â”‚ â”‚ ctx, span := tracer.Start(ctx, "handleRequest")       â”‚ â”‚
â”‚ â”‚ defer span.End()                                      â”‚ â”‚
â”‚ â”‚                                                       â”‚ â”‚
â”‚ â”‚ // è‡ªå‹•è¨ˆè£…: HTTP client, DB, gRPC                    â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ OTLP (gRPC/HTTP)
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenTelemetry Collector (DaemonSet/Sidecar)              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Receiver â†’ Processor â†’ Exporter                       â”‚ â”‚
â”‚ â”‚                                                       â”‚ â”‚
â”‚ â”‚ ãƒ»ãƒãƒƒãƒå‡¦ç†                                          â”‚ â”‚
â”‚ â”‚ ãƒ»ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°                                        â”‚ â”‚
â”‚ â”‚ ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°                                      â”‚ â”‚
â”‚ â”‚ ãƒ»å±æ€§ã®è¿½åŠ /å‰Šé™¤                                     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚                 â”‚
        â–¼                 â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Jaeger       â”‚  â”‚ Tempo        â”‚  â”‚ Zipkin       â”‚
â”‚ (UIã‚ã‚Š)     â”‚  â”‚ (Grafanaé€£æº)â”‚  â”‚ (ãƒ¬ã‚¬ã‚·ãƒ¼)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### OpenTelemetry Collector ã®è¨­å®š

```yaml
# OpenTelemetry Collector ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
  namespace: observability
data:
  config.yaml: |
    receivers:
      # OTLP receiver (gRPC)
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318

      # Jaeger receiver (å¾Œæ–¹äº’æ›æ€§)
      jaeger:
        protocols:
          grpc:
            endpoint: 0.0.0.0:14250
          thrift_http:
            endpoint: 0.0.0.0:14268

      # Zipkin receiver
      zipkin:
        endpoint: 0.0.0.0:9411

    processors:
      # ãƒãƒƒãƒå‡¦ç†ï¼ˆåŠ¹ç‡åŒ–ï¼‰
      batch:
        timeout: 10s
        send_batch_size: 1024

      # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆæœ¬ç•ªç’°å¢ƒã§å¿…é ˆï¼‰
      probabilistic_sampler:
        sampling_percentage: 10  # 10%ã®traceã®ã¿ä¿å­˜

      # Tail samplingï¼ˆã‚¨ãƒ©ãƒ¼ã®traceã¯100%ä¿å­˜ï¼‰
      tail_sampling:
        policies:
        # ã‚¨ãƒ©ãƒ¼ã¯å¿…ãšä¿å­˜
        - name: error-policy
          type: status_code
          status_code:
            status_codes: [ERROR]
        # é…ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯å¿…ãšä¿å­˜
        - name: slow-requests
          type: latency
          latency:
            threshold_ms: 1000
        # ãã‚Œä»¥å¤–ã¯10%
        - name: probabilistic-policy
          type: probabilistic
          probabilistic:
            sampling_percentage: 10

      # Kuberneteså±æ€§ã®è¿½åŠ 
      k8sattributes:
        auth_type: "serviceAccount"
        passthrough: false
        extract:
          metadata:
          - k8s.namespace.name
          - k8s.deployment.name
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.node.name
          labels:
          - tag_name: app
            key: app
            from: pod

      # ãƒªã‚½ãƒ¼ã‚¹å±æ€§ã®è¿½åŠ 
      resource:
        attributes:
        - key: cluster.name
          value: production-eks
          action: upsert
        - key: environment
          value: production
          action: upsert

    exporters:
      # Jaeger exporter
      jaeger:
        endpoint: jaeger-collector.observability.svc:14250
        tls:
          insecure: true

      # Tempo exporter (Grafana)
      otlp/tempo:
        endpoint: tempo.observability.svc:4317
        tls:
          insecure: true

      # ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
      logging:
        loglevel: info

    service:
      pipelines:
        traces:
          receivers: [otlp, jaeger, zipkin]
          processors: [k8sattributes, resource, tail_sampling, batch]
          exporters: [jaeger, otlp/tempo]

---
# OpenTelemetry Collector Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
  namespace: observability
spec:
  replicas: 3
  selector:
    matchLabels:
      app: otel-collector
  template:
    metadata:
      labels:
        app: otel-collector
    spec:
      serviceAccountName: otel-collector
      containers:
      - name: otel-collector
        image: otel/opentelemetry-collector-contrib:0.91.0
        args:
        - --config=/conf/config.yaml
        ports:
        - containerPort: 4317  # OTLP gRPC
        - containerPort: 4318  # OTLP HTTP
        - containerPort: 14250 # Jaeger gRPC
        - containerPort: 9411  # Zipkin
        volumeMounts:
        - name: config
          mountPath: /conf
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
          limits:
            cpu: 1000m
            memory: 512Mi
      volumes:
      - name: config
        configMap:
          name: otel-collector-config
```

#### ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã®è¨ˆè£…ï¼ˆGoä¾‹ï¼‰

```go
package main

import (
    "context"
    "log"
    "time"

    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/attribute"
    "go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc"
    "go.opentelemetry.io/otel/sdk/resource"
    sdktrace "go.opentelemetry.io/otel/sdk/trace"
    semconv "go.opentelemetry.io/otel/semconv/v1.21.0"
    "go.opentelemetry.io/otel/trace"
    "google.golang.org/grpc"
    "google.golang.org/grpc/credentials/insecure"
)

func initTracer() (*sdktrace.TracerProvider, error) {
    // OTLP exporter ã®è¨­å®š
    ctx := context.Background()

    conn, err := grpc.DialContext(
        ctx,
        "otel-collector.observability.svc:4317",
        grpc.WithTransportCredentials(insecure.NewCredentials()),
        grpc.WithBlock(),
    )
    if err != nil {
        return nil, err
    }

    exporter, err := otlptracegrpc.New(ctx, otlptracegrpc.WithGRPCConn(conn))
    if err != nil {
        return nil, err
    }

    // Resource ã®å®šç¾©ï¼ˆã‚µãƒ¼ãƒ“ã‚¹è­˜åˆ¥æƒ…å ±ï¼‰
    res, err := resource.New(ctx,
        resource.WithAttributes(
            semconv.ServiceName("myapp"),
            semconv.ServiceVersion("1.2.3"),
            semconv.DeploymentEnvironment("production"),
            attribute.String("k8s.namespace", "prod"),
        ),
    )
    if err != nil {
        return nil, err
    }

    // Tracer Provider ã®ä½œæˆ
    tp := sdktrace.NewTracerProvider(
        sdktrace.WithBatcher(exporter),
        sdktrace.WithResource(res),
        // ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è¨­å®š
        sdktrace.WithSampler(sdktrace.TraceIDRatioBased(0.1)), // 10%
    )

    otel.SetTracerProvider(tp)
    return tp, nil
}

func main() {
    tp, err := initTracer()
    if err != nil {
        log.Fatal(err)
    }
    defer func() {
        if err := tp.Shutdown(context.Background()); err != nil {
            log.Printf("Error shutting down tracer provider: %v", err)
        }
    }()

    // Tracer ã®å–å¾—
    tracer := otel.Tracer("myapp")

    // Span ã®ä½œæˆ
    ctx := context.Background()
    ctx, span := tracer.Start(ctx, "main.handleRequest")
    defer span.End()

    // å±æ€§ã®è¿½åŠ 
    span.SetAttributes(
        attribute.String("user.id", "user123"),
        attribute.String("http.method", "POST"),
        attribute.String("http.route", "/api/v1/orders"),
    )

    // å­Spanã®ä½œæˆ
    processOrder(ctx, tracer)

    // ã‚¤ãƒ™ãƒ³ãƒˆã®è¨˜éŒ²
    span.AddEvent("Order processed successfully")
}

func processOrder(ctx context.Context, tracer trace.Tracer) {
    ctx, span := tracer.Start(ctx, "processOrder")
    defer span.End()

    // ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒª
    queryDB(ctx, tracer)

    // å¤–éƒ¨APIå‘¼ã³å‡ºã—
    callExternalAPI(ctx, tracer)
}

func queryDB(ctx context.Context, tracer trace.Tracer) {
    ctx, span := tracer.Start(ctx, "database.query")
    defer span.End()

    span.SetAttributes(
        attribute.String("db.system", "postgresql"),
        attribute.String("db.statement", "SELECT * FROM orders WHERE id = $1"),
    )

    // ã‚¯ã‚¨ãƒªå®Ÿè¡Œï¼ˆç–‘ä¼¼ï¼‰
    time.Sleep(50 * time.Millisecond)
}

func callExternalAPI(ctx context.Context, tracer trace.Tracer) {
    ctx, span := tracer.Start(ctx, "http.client.request")
    defer span.End()

    span.SetAttributes(
        attribute.String("http.url", "https://api.example.com/payment"),
        attribute.String("http.method", "POST"),
    )

    // APIå‘¼ã³å‡ºã—ï¼ˆç–‘ä¼¼ï¼‰
    time.Sleep(100 * time.Millisecond)

    span.SetAttributes(
        attribute.Int("http.status_code", 200),
    )
}
```

---

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯å¯è¦³æ¸¬æ€§ã®ä¸»è¦éƒ¨åˆ†ã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã¾ã™ã€‚ç¶šã‘ã¦ã€Œç½å®³å¾©æ—§ï¼ˆDisaster Recoveryï¼‰ã€ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

## ğŸ“š å‚è€ƒãƒªã‚½ãƒ¼ã‚¹

- [Fluent Bit Documentation](https://docs.fluentbit.io/)
- [Prometheus Operator](https://prometheus-operator.dev/)
- [Grafana Loki](https://grafana.com/oss/loki/)
- [OpenTelemetry](https://opentelemetry.io/)
- [Jaeger](https://www.jaegertracing.io/)
