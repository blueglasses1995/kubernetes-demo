# Kubernetesæœ¬ç•ªç’°å¢ƒ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°å®Œå…¨ã‚¬ã‚¤ãƒ‰

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€Kubernetesæœ¬ç•ªç’°å¢ƒã§ç™ºç”Ÿã™ã‚‹ä¸€èˆ¬çš„ãªå•é¡Œã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’ã€æŠ€è¡“çš„åŸç†ã‹ã‚‰å®Ÿè·µã¾ã§å¾¹åº•çš„ã«è§£èª¬ã—ã¾ã™ã€‚

## ğŸ“š ç›®æ¬¡

1. [ãƒ‡ãƒãƒƒã‚°ãƒ„ãƒ¼ãƒ«](#1-ãƒ‡ãƒãƒƒã‚°ãƒ„ãƒ¼ãƒ«)
2. [ä¸€èˆ¬çš„ãªå•é¡Œ](#2-ä¸€èˆ¬çš„ãªå•é¡Œ)
3. [ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œ](#3-ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œ)
4. [ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å•é¡Œ](#4-ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å•é¡Œ)

---

## 1. ãƒ‡ãƒãƒƒã‚°ãƒ„ãƒ¼ãƒ«

### 1.1 kubectl debug

#### Ephemeral Containers ã‚’ä½¿ã£ãŸãƒ‡ãƒãƒƒã‚°

```bash
# å®Ÿè¡Œä¸­ã®Podã«ãƒ‡ãƒãƒƒã‚°ã‚³ãƒ³ãƒ†ãƒŠã‚’è¿½åŠ ï¼ˆKubernetes 1.25+ï¼‰
kubectl debug -it pod-name --image=busybox --target=container-name

# ä¾‹: nginxã‚³ãƒ³ãƒ†ãƒŠã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‚’èª¿æŸ»
kubectl debug -it nginx-pod --image=busybox:1.36 --target=nginx

# ãƒ‡ãƒãƒƒã‚°ã‚³ãƒ³ãƒ†ãƒŠå†…ã§:
/ # ls -la /usr/share/nginx/html/
/ # ps aux
/ # netstat -tunlp

# Nodeä¸Šã§ç›´æ¥ãƒ‡ãƒãƒƒã‚°ï¼ˆhostãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€hostPIDï¼‰
kubectl debug node/node-1 -it --image=ubuntu:22.04

# Nodeä¸Šã§rootæ¨©é™ã§ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ
root@node-1:/# chroot /host
root@node-1:/# systemctl status kubelet
root@node-1:/# journalctl -u kubelet -n 100
```

#### ãƒ‡ãƒãƒƒã‚°ç”¨Podã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

```yaml
# debug-pod.yaml - å®Œå…¨è£…å‚™ã®ãƒ‡ãƒãƒƒã‚°Pod
apiVersion: v1
kind: Pod
metadata:
  name: debug-tools
  namespace: default
spec:
  # ç‰¹å®šãƒãƒ¼ãƒ‰ã§å®Ÿè¡Œ
  nodeName: node-1

  # ãƒ›ã‚¹ãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨
  hostNetwork: true
  hostPID: true
  hostIPC: true

  containers:
  - name: debug
    image: nicolaka/netshoot:latest  # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‡ãƒãƒƒã‚°ãƒ„ãƒ¼ãƒ«æº€è¼‰
    command: ["/bin/bash"]
    args: ["-c", "sleep infinity"]

    securityContext:
      privileged: true

    volumeMounts:
    # ãƒ›ã‚¹ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‚’ãƒã‚¦ãƒ³ãƒˆ
    - name: host-root
      mountPath: /host
      readOnly: true

  volumes:
  - name: host-root
    hostPath:
      path: /
      type: Directory

  # å®Œäº†å¾Œã‚‚æ®‹ã™ï¼ˆæ‰‹å‹•å‰Šé™¤ã¾ã§ï¼‰
  restartPolicy: Never
```

```bash
# ãƒ‡ãƒãƒƒã‚°Podã®ä½¿ç”¨ä¾‹
kubectl apply -f debug-pod.yaml
kubectl exec -it debug-tools -- bash

# ã‚³ãƒ³ãƒ†ãƒŠå†…ã§åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«:
# - tcpdump: ãƒ‘ã‚±ãƒƒãƒˆã‚­ãƒ£ãƒ—ãƒãƒ£
# - nmap: ãƒãƒ¼ãƒˆã‚¹ã‚­ãƒ£ãƒ³
# - curl, wget: HTTP ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
# - dig, nslookup: DNS ã‚¯ã‚¨ãƒª
# - netstat, ss: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šçŠ¶æ…‹
# - iperf3: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¸¯åŸŸæ¸¬å®š
# - ethtool: NICè¨­å®šç¢ºèª

# ä¾‹: ç‰¹å®šã‚µãƒ¼ãƒ“ã‚¹ã¸ã®æ¥ç¶šç¢ºèª
bash-5.1# curl -v http://myapp.prod.svc.cluster.local:8080/health

# ä¾‹: DNSè§£æ±ºç¢ºèª
bash-5.1# dig myapp.prod.svc.cluster.local

# ä¾‹: ãƒ‘ã‚±ãƒƒãƒˆã‚­ãƒ£ãƒ—ãƒãƒ£
bash-5.1# tcpdump -i eth0 -nn 'port 8080'
```

### 1.2 kubectl-sniff (ãƒ‘ã‚±ãƒƒãƒˆã‚­ãƒ£ãƒ—ãƒãƒ£)

```bash
# kubectl-sniff ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
kubectl krew install sniff

# Pod ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£
kubectl sniff pod-name -n namespace -o capture.pcap

# ç‰¹å®šã®ã‚³ãƒ³ãƒ†ãƒŠã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
kubectl sniff pod-name -c container-name -n namespace

# ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨
kubectl sniff pod-name -f "port 8080" -n namespace

# Wiresharkã§ç›´æ¥é–‹ã
kubectl sniff pod-name -n namespace -o - | wireshark -k -i -
```

### 1.3 k9sï¼ˆTUIãƒ„ãƒ¼ãƒ«ï¼‰

```bash
# k9s ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
brew install derailed/k9s/k9s

# èµ·å‹•
k9s

# ä¸»è¦ãªã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆ:
# 0: ã™ã¹ã¦ã®Namespaceã‚’è¡¨ç¤º
# :pods: Podä¸€è¦§
# :svc: Serviceä¸€è¦§
# :deploy: Deploymentä¸€è¦§
# d: è©³ç´°è¡¨ç¤º
# l: ãƒ­ã‚°è¡¨ç¤º
# e: ç·¨é›†
# shift+f: ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰
# ctrl+k: killï¼ˆå‰Šé™¤ï¼‰
# /: ãƒ•ã‚£ãƒ«ã‚¿
# ?: ãƒ˜ãƒ«ãƒ—

# Podã®ãƒ­ã‚°ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒ è¡¨ç¤º
# 1. :pods ã§Podä¸€è¦§
# 2. å¯¾è±¡Podã‚’é¸æŠ
# 3. l ã‚­ãƒ¼ã§ãƒ­ã‚°è¡¨ç¤º
# 4. 0-9 ã‚­ãƒ¼ã§å‰ã®ãƒ­ã‚°ã‚’è¡¨ç¤º
```

---

## 2. ä¸€èˆ¬çš„ãªå•é¡Œ

### 2.1 ImagePullBackOff

#### åŸå› ã¨è¨ºæ–­

```bash
# Podã®çŠ¶æ…‹ç¢ºèª
kubectl get pods
# NAME                     READY   STATUS             RESTARTS   AGE
# myapp-7d9f8b4c5d-abc12   0/1     ImagePullBackOff   0          2m

# ã‚¤ãƒ™ãƒ³ãƒˆç¢ºèª
kubectl describe pod myapp-7d9f8b4c5d-abc12

# ç¢ºèªã™ã¹ãã‚¤ãƒ™ãƒ³ãƒˆ:
# Events:
#   Type     Reason     Age                  From               Message
#   ----     ------     ----                 ----               -------
#   Normal   Scheduled  2m                   default-scheduler  Successfully assigned default/myapp-7d9f8b4c5d-abc12 to node-1
#   Normal   Pulling    30s (x4 over 2m)     kubelet            Pulling image "myregistry.io/myapp:v1.2.3"
#   Warning  Failed     29s (x4 over 2m)     kubelet            Failed to pull image "myregistry.io/myapp:v1.2.3": rpc error: code = Unknown desc = Error response from daemon: pull access denied for myregistry.io/myapp, repository does not exist or may require 'docker login'
#   Warning  Failed     29s (x4 over 2m)     kubelet            Error: ErrImagePull
#   Normal   BackOff    15s (x6 over 2m)     kubelet            Back-off pulling image "myregistry.io/myapp:v1.2.3"
#   Warning  Failed     15s (x6 over 2m)     kubelet            Error: ImagePullBackOff
```

#### åŸå› åˆ¥ã®å¯¾å‡¦æ³•

```yaml
# åŸå› 1: ã‚¤ãƒ¡ãƒ¼ã‚¸ãŒå­˜åœ¨ã—ãªã„
# å¯¾å‡¦: ã‚¤ãƒ¡ãƒ¼ã‚¸åã¨ã‚¿ã‚°ã‚’ç¢ºèª

# ãƒ­ãƒ¼ã‚«ãƒ«ã§ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ç¢ºèª
docker pull myregistry.io/myapp:v1.2.3

# ã‚¿ã‚°ä¸€è¦§ã‚’ç¢ºèªï¼ˆãƒ¬ã‚¸ã‚¹ãƒˆãƒªAPIï¼‰
curl -X GET https://myregistry.io/v2/myapp/tags/list

---
# åŸå› 2: ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒ¬ã‚¸ã‚¹ãƒˆãƒªã®èªè¨¼å¤±æ•—
# å¯¾å‡¦: ImagePullSecret ã‚’ä½œæˆ

# Docker credentialsã‹ã‚‰Secretã‚’ä½œæˆ
kubectl create secret docker-registry regcred \
  --docker-server=myregistry.io \
  --docker-username=myuser \
  --docker-password=mypassword \
  --docker-email=myemail@example.com

# Podã§ä½¿ç”¨
apiVersion: v1
kind: Pod
metadata:
  name: myapp
spec:
  containers:
  - name: app
    image: myregistry.io/myapp:v1.2.3
  imagePullSecrets:
  - name: regcred  # â† ImagePullSecretã‚’æŒ‡å®š

---
# åŸå› 3: ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆï¼ˆDocker Hubï¼‰
# å¯¾å‡¦: èªè¨¼ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€åˆ¥ã®ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‚’ä½¿ç”¨

# Docker Hubã®èªè¨¼æƒ…å ±ã§Secretä½œæˆ
kubectl create secret docker-registry dockerhub \
  --docker-server=https://index.docker.io/v1/ \
  --docker-username=myuser \
  --docker-password=mypassword

---
# åŸå› 4: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å•é¡Œ
# å¯¾å‡¦: Nodeã‹ã‚‰ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã¸ã®æ¥ç¶šç¢ºèª

# Nodeä¸Šã§ç¢ºèª
kubectl debug node/node-1 -it --image=curlimages/curl
# curl -v https://myregistry.io/v2/

# ãƒ—ãƒ­ã‚­ã‚·è¨­å®šã®ç¢ºèª
kubectl get cm -n kube-system kube-proxy -o yaml | grep -i proxy
```

### 2.2 CrashLoopBackOff

#### è¨ºæ–­ãƒ•ãƒ­ãƒ¼

```bash
# PodçŠ¶æ…‹ç¢ºèª
kubectl get pods
# NAME                     READY   STATUS             RESTARTS   AGE
# myapp-7d9f8b4c5d-xyz     0/1     CrashLoopBackOff   5          10m

# ãƒ­ã‚°ç¢ºèªï¼ˆç¾åœ¨ã®ã‚³ãƒ³ãƒ†ãƒŠï¼‰
kubectl logs myapp-7d9f8b4c5d-xyz

# å‰å›ã‚¯ãƒ©ãƒƒã‚·ãƒ¥æ™‚ã®ãƒ­ã‚°ç¢ºèª
kubectl logs myapp-7d9f8b4c5d-xyz --previous

# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°ã‚¹ãƒˆãƒªãƒ¼ãƒ 
kubectl logs -f myapp-7d9f8b4c5d-xyz

# è©³ç´°æƒ…å ±
kubectl describe pod myapp-7d9f8b4c5d-xyz
```

#### åŸå› åˆ¥ã®å¯¾å‡¦æ³•

```yaml
# åŸå› 1: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼
# å¯¾å‡¦: ãƒ­ã‚°ã‹ã‚‰ã‚¨ãƒ©ãƒ¼ã‚’ç‰¹å®š

# ä¾‹: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼
# Logs:
# Error: connect ECONNREFUSED 10.100.1.5:5432
# at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1144:16)

# ç¢ºèªäº‹é …:
# - ServiceãŒå­˜åœ¨ã™ã‚‹ã‹
# - EndpointãŒå­˜åœ¨ã™ã‚‹ã‹
# - ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒãƒªã‚·ãƒ¼ã§è¨±å¯ã•ã‚Œã¦ã„ã‚‹ã‹

kubectl get svc postgres
kubectl get endpoints postgres
kubectl get networkpolicies

---
# åŸå› 2: Liveness Probe ã®å¤±æ•—
# å¯¾å‡¦: Probeã®è¨­å®šã‚’è¦‹ç›´ã—

apiVersion: v1
kind: Pod
metadata:
  name: myapp
spec:
  containers:
  - name: app
    image: myapp:1.0
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8080
      initialDelaySeconds: 30  # ã‚¢ãƒ—ãƒªèµ·å‹•ã‚’å¾…ã¤ï¼ˆçŸ­ã™ãã‚‹ã¨å³åº§ã«å¤±æ•—ï¼‰
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 3      # 3å›é€£ç¶šå¤±æ•—ã§ã‚³ãƒ³ãƒ†ãƒŠã‚’å†èµ·å‹•

# Probeå¤±æ•—ã®ç¢ºèª
kubectl describe pod myapp | grep -A 10 "Liveness"
# Liveness:       http-get http://:8080/healthz delay=30s timeout=5s period=10s #success=1 #failure=3
# Warning  Unhealthy  2m (x5 over 3m)  kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500

---
# åŸå› 3: OOMKilled
# å¯¾å‡¦: ãƒ¡ãƒ¢ãƒªåˆ¶é™ã‚’å¢—ã‚„ã™

# OOMã®ç¢ºèª
kubectl describe pod myapp | grep -i oom
# Last State:     Terminated
#   Reason:       OOMKilled
#   Exit Code:    137

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç¢ºèª
kubectl top pod myapp

# ãƒ¡ãƒ¢ãƒªåˆ¶é™ã‚’å¢—ã‚„ã™
apiVersion: v1
kind: Pod
metadata:
  name: myapp
spec:
  containers:
  - name: app
    image: myapp:1.0
    resources:
      requests:
        memory: "256Mi"
      limits:
        memory: "1Gi"  # å¢—ã‚„ã™

---
# åŸå› 4: è¨­å®šãƒŸã‚¹ï¼ˆç’°å¢ƒå¤‰æ•°ã€ConfigMapç­‰ï¼‰
# å¯¾å‡¦: è¨­å®šã‚’ç¢ºèª

# Podå†…ã®ç’°å¢ƒå¤‰æ•°ç¢ºèª
kubectl exec myapp -- env

# ConfigMapã®å†…å®¹ç¢ºèª
kubectl get cm myapp-config -o yaml

# Secretã®å†…å®¹ç¢ºèªï¼ˆbase64ãƒ‡ã‚³ãƒ¼ãƒ‰ï¼‰
kubectl get secret myapp-secret -o json | jq -r '.data | to_entries[] | .key + ": " + (.value | @base64d)'
```

### 2.3 Pending Pods

#### è¨ºæ–­

```bash
# Pending Pod ã®ç¢ºèª
kubectl get pods
# NAME                     READY   STATUS    RESTARTS   AGE
# myapp-7d9f8b4c5d-pending 0/1     Pending   0          5m

# ã‚¤ãƒ™ãƒ³ãƒˆç¢ºèª
kubectl describe pod myapp-7d9f8b4c5d-pending

# Pendingã®ä¸€èˆ¬çš„ãªåŸå› :
# 1. ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³ï¼ˆCPU/ãƒ¡ãƒ¢ãƒªï¼‰
# 2. PVCãŒãƒã‚¤ãƒ³ãƒ‰ã•ã‚Œã¦ã„ãªã„
# 3. Node Selectorã«ãƒãƒƒãƒã™ã‚‹ãƒãƒ¼ãƒ‰ãŒãªã„
# 4. Taints/Tolerations ã®ä¸ä¸€è‡´
# 5. PodDisruptionBudget ã«ã‚ˆã‚‹åˆ¶é™
```

#### åŸå› åˆ¥ã®å¯¾å‡¦æ³•

```bash
# åŸå› 1: ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³
# Events:
# Warning  FailedScheduling  1m (x10 over 5m)  default-scheduler  0/3 nodes are available: 3 Insufficient cpu.

# ãƒãƒ¼ãƒ‰ã®ãƒªã‚½ãƒ¼ã‚¹ç¢ºèª
kubectl describe nodes | grep -A 5 "Allocated resources"

# å‡ºåŠ›ä¾‹:
# Allocated resources:
#   (Total limits may be over 100 percent, i.e., overcommitted.)
#   Resource           Requests      Limits
#   --------           --------      ------
#   cpu                7500m (94%)   8000m (100%)
#   memory             24Gi (77%)    30Gi (96%)

# å¯¾å‡¦:
# - ä¸è¦ãªPodã‚’å‰Šé™¤
# - ãƒãƒ¼ãƒ‰ã‚’è¿½åŠ 
# - Pod ã®ãƒªã‚½ãƒ¼ã‚¹è¦æ±‚ã‚’æ¸›ã‚‰ã™

---
# åŸå› 2: PVCãŒãƒã‚¤ãƒ³ãƒ‰ã•ã‚Œã¦ã„ãªã„
# Events:
# Warning  FailedScheduling  1m (x10 over 5m)  default-scheduler  persistentvolumeclaim "myapp-data" not found

# PVCçŠ¶æ…‹ç¢ºèª
kubectl get pvc
# NAME         STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
# myapp-data   Pending   -        -          -              fast-ssd       5m

# PVCã®è©³ç´°ç¢ºèª
kubectl describe pvc myapp-data

# Events:
# Warning  ProvisioningFailed  1m (x5 over 5m)  persistentvolume-controller  storageclass.storage.k8s.io "fast-ssd" not found

# å¯¾å‡¦:
# - StorageClass ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
# - PV ãŒåˆ©ç”¨å¯èƒ½ã‹ç¢ºèªï¼ˆStatic Provisioning ã®å ´åˆï¼‰

kubectl get sc
kubectl get pv

---
# åŸå› 3: Node Selector ä¸ä¸€è‡´
# Events:
# Warning  FailedScheduling  1m (x10 over 5m)  default-scheduler  0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector.

# Pod ã® nodeSelector ç¢ºèª
kubectl get pod myapp-7d9f8b4c5d-pending -o yaml | grep -A 3 nodeSelector

# ãƒãƒ¼ãƒ‰ã®ãƒ©ãƒ™ãƒ«ç¢ºèª
kubectl get nodes --show-labels

# å¯¾å‡¦:
# - nodeSelector ã‚’ä¿®æ­£
# - ãƒãƒ¼ãƒ‰ã«é©åˆ‡ãªãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ 

kubectl label node node-1 disktype=ssd

---
# åŸå› 4: Taints/Tolerations
# Events:
# Warning  FailedScheduling  1m (x10 over 5m)  default-scheduler  0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 node(s) had untolerated taint {node-role.kubernetes.io/master: }.

# ãƒãƒ¼ãƒ‰ã®Taintsç¢ºèª
kubectl describe node node-1 | grep Taints

# Tolerationè¿½åŠ 
apiVersion: v1
kind: Pod
metadata:
  name: myapp
spec:
  tolerations:
  - key: "key1"
    operator: "Equal"
    value: "value1"
    effect: "NoSchedule"
  containers:
  - name: app
    image: myapp:1.0
```

### 2.4 PVC Binding Failures

```bash
# PVC ãŒ Pending çŠ¶æ…‹
kubectl get pvc
# NAME         STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
# myapp-data   Pending   -        -          -              gp2            10m

# è©³ç´°ç¢ºèª
kubectl describe pvc myapp-data

# åŸå› 1: StorageClass ãŒå­˜åœ¨ã—ãªã„
# Events:
# Warning  ProvisioningFailed  1m (x5 over 10m)  persistentvolume-controller  storageclass.storage.k8s.io "gp2" not found

# StorageClass ä¸€è¦§ç¢ºèª
kubectl get sc

# StorageClass ä½œæˆï¼ˆAWS EBSä¾‹ï¼‰
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gp2
provisioner: ebs.csi.aws.com
parameters:
  type: gp2
  fsType: ext4
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true

---
# åŸå› 2: CSI Driver ãŒå‹•ä½œã—ã¦ã„ãªã„
# CSI Controller Podã®ç¢ºèª
kubectl get pods -n kube-system | grep csi

# å‡ºåŠ›ä¾‹:
# ebs-csi-controller-7d9f8b-abc12     6/6     Running   0          30d
# ebs-csi-controller-7d9f8b-def34     6/6     Running   0          30d
# ebs-csi-node-xyz                    3/3     Running   0          30d

# CSI Driverãƒ­ã‚°ç¢ºèª
kubectl logs -n kube-system ebs-csi-controller-7d9f8b-abc12 -c ebs-plugin

---
# åŸå› 3: ã‚¯ã‚©ãƒ¼ã‚¿è¶…éï¼ˆã‚¯ãƒ©ã‚¦ãƒ‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼‰
# Events:
# Warning  ProvisioningFailed  1m  persistentvolume-controller  Failed to provision volume with StorageClass "gp2": rpc error: code = ResourceExhausted desc = volume limit exceeded

# AWS ã®å ´åˆã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã”ã¨ã®EBSãƒœãƒªãƒ¥ãƒ¼ãƒ æ•°åˆ¶é™
# https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/volume_limits.html

# å¯¾å‡¦:
# - ä¸è¦ãªãƒœãƒªãƒ¥ãƒ¼ãƒ ã‚’å‰Šé™¤
# - ã‚ˆã‚Šå¤§ãã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã«å¤‰æ›´
```

---

## 3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œ

### 3.1 High CPU Usage

#### è¨ºæ–­

```bash
# Pod ã® CPU ä½¿ç”¨ç‡ç¢ºèª
kubectl top pods --all-namespaces --sort-by=cpu

# ä¸Šä½10ä»¶
kubectl top pods --all-namespaces --sort-by=cpu | head -n 11

# ç‰¹å®šPodã®è©³ç´°
kubectl top pod myapp-7d9f8b4c5d-xyz --containers

# å‡ºåŠ›:
# POD                       NAME     CPU(cores)   MEMORY(bytes)
# myapp-7d9f8b4c5d-xyz      app      950m         512Mi
# myapp-7d9f8b4c5d-xyz      sidecar  50m          64Mi

# CPUä½¿ç”¨ç‡ãŒé«˜ã„å ´åˆã®ç¢ºèªäº‹é …

# 1. CPU Throttling ã®ç¢ºèª
kubectl describe pod myapp-7d9f8b4c5d-xyz

# Containerå†…ã§throttlingçŠ¶æ…‹ã‚’ç¢ºèª
kubectl exec myapp-7d9f8b4c5d-xyz -- cat /sys/fs/cgroup/cpu/cpu.stat

# throttled_time ãŒå¢—åŠ ã—ã¦ã„ã‚‹å ´åˆã€CPUåˆ¶é™ã«é”ã—ã¦ã„ã‚‹
```

#### å¯¾å‡¦æ³•

```yaml
# 1. CPU Limits ã‚’å¢—ã‚„ã™
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  template:
    spec:
      containers:
      - name: app
        image: myapp:1.0
        resources:
          requests:
            cpu: 500m
          limits:
            cpu: 2000m  # å¢—ã‚„ã™

---
# 2. Horizontal Pod Autoscaler (HPA) ã‚’è¨­å®š
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: myapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # CPU 70%ã§ã‚¹ã‚±ãƒ¼ãƒ«

  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Pods
        value: 1
        periodSeconds: 60

---
# 3. ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨

# Go ã‚¢ãƒ—ãƒªã®å ´åˆ: pprof
# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã« pprof ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’è¿½åŠ 
import _ "net/http/pprof"

# ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰
kubectl port-forward pod/myapp-7d9f8b4c5d-xyz 6060:6060

# CPU ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
go tool pprof http://localhost:6060/debug/pprof/profile?seconds=30

# Python ã‚¢ãƒ—ãƒªã®å ´åˆ: py-spy
kubectl exec -it myapp-7d9f8b4c5d-xyz -- py-spy top --pid 1
```

### 3.2 High Memory Usage / OOMKilled

#### è¨ºæ–­

```bash
# OOM ã§çµ‚äº†ã—ãŸPodã®ç¢ºèª
kubectl get pods --all-namespaces --field-selector status.phase=Failed

# Podè©³ç´°ç¢ºèª
kubectl describe pod myapp-7d9f8b4c5d-oom

# Last State:
#   Terminated:
#     Reason:       OOMKilled
#     Exit Code:    137
#     Started:      Mon, 15 Jan 2024 10:00:00 +0900
#     Finished:     Mon, 15 Jan 2024 10:05:23 +0900

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç¢ºèª
kubectl top pod myapp-7d9f8b4c5d-xyz

# Nodeã®ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ç¢ºèª
kubectl describe node node-1 | grep -A 5 "MemoryPressure"

# Containerå†…ã®ãƒ¡ãƒ¢ãƒªçµ±è¨ˆ
kubectl exec myapp-7d9f8b4c5d-xyz -- cat /sys/fs/cgroup/memory/memory.stat
```

#### ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯èª¿æŸ»

```bash
# Go ã‚¢ãƒ—ãƒªã®å ´åˆ
kubectl port-forward pod/myapp-7d9f8b4c5d-xyz 6060:6060

# ãƒ’ãƒ¼ãƒ—ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
go tool pprof http://localhost:6060/debug/pprof/heap

# ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã§åˆ†æ
(pprof) top10
(pprof) list <function-name>

# Java ã‚¢ãƒ—ãƒªã®å ´åˆ
kubectl exec -it myapp-7d9f8b4c5d-xyz -- jmap -heap 1

# ãƒ’ãƒ¼ãƒ—ãƒ€ãƒ³ãƒ—å–å¾—
kubectl exec -it myapp-7d9f8b4c5d-xyz -- jmap -dump:format=b,file=/tmp/heap.bin 1

# ãƒ’ãƒ¼ãƒ—ãƒ€ãƒ³ãƒ—ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«ã‚³ãƒ”ãƒ¼
kubectl cp myapp-7d9f8b4c5d-xyz:/tmp/heap.bin ./heap.bin

# Eclipse MAT ç­‰ã§åˆ†æ
```

#### å¯¾å‡¦æ³•

```yaml
# 1. ãƒ¡ãƒ¢ãƒªåˆ¶é™ã‚’å¢—ã‚„ã™
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  template:
    spec:
      containers:
      - name: app
        image: myapp:1.0
        resources:
          requests:
            memory: "512Mi"
          limits:
            memory: "2Gi"  # å¢—ã‚„ã™

---
# 2. Vertical Pod Autoscaler (VPA) ã‚’ä½¿ç”¨
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: myapp-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  updatePolicy:
    updateMode: "Auto"  # è‡ªå‹•çš„ã«ãƒªã‚½ãƒ¼ã‚¹è¦æ±‚ã‚’èª¿æ•´
  resourcePolicy:
    containerPolicies:
    - containerName: app
      minAllowed:
        memory: "256Mi"
      maxAllowed:
        memory: "4Gi"

---
# 3. ãƒ¡ãƒ¢ãƒªãƒªãƒŸãƒƒãƒˆãªã—ï¼ˆrequestsã®ã¿ï¼‰
# OOMã‚ˆã‚Šã‚‚ã€Nodeã®ãƒ¡ãƒ¢ãƒªä¸è¶³ã§evictionã•ã‚Œã‚‹æ–¹ãŒãƒã‚·ãªå ´åˆ
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  template:
    spec:
      containers:
      - name: app
        image: myapp:1.0
        resources:
          requests:
            memory: "512Mi"
          # limits ã‚’è¨­å®šã—ãªã„
```

### 3.3 Disk I/O Bottlenecks

```bash
# Node ã® I/O ç¢ºèª
kubectl debug node/node-1 -it --image=ubuntu:22.04
chroot /host

# iostat ã§ãƒ‡ã‚£ã‚¹ã‚¯I/Oç¢ºèª
apt-get update && apt-get install -y sysstat
iostat -x 1

# å‡ºåŠ›:
# Device   r/s    w/s    rkB/s    wkB/s  %util
# nvme0n1  120.0  450.0  4800.0   18000.0  98.5  â† é«˜è² è·

# ã©ã®ãƒ—ãƒ­ã‚»ã‚¹ãŒI/Oã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã‹
iotop -o

# Podå†…ã§ã®I/Oç¢ºèª
kubectl exec -it myapp-7d9f8b4c5d-xyz -- sh

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®I/Oçµ±è¨ˆ
cat /proc/self/io
# rchar: 1234567890      # èª­ã¿å–ã‚Šãƒã‚¤ãƒˆæ•°
# wchar: 9876543210      # æ›¸ãè¾¼ã¿ãƒã‚¤ãƒˆæ•°
# syscr: 123456          # read() ã‚·ã‚¹ãƒ†ãƒ ã‚³ãƒ¼ãƒ«å›æ•°
# syscw: 654321          # write() ã‚·ã‚¹ãƒ†ãƒ ã‚³ãƒ¼ãƒ«å›æ•°
```

#### å¯¾å‡¦æ³•

```yaml
# 1. ã‚ˆã‚Šé«˜é€Ÿãªã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myapp-data
spec:
  storageClassName: fast-ssd  # io1, gp3 ç­‰ã®é«˜é€Ÿã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi

---
# AWS EBS io2 ã®ä¾‹
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
provisioner: ebs.csi.aws.com
parameters:
  type: io2
  iopsPerGB: "50"     # 50 IOPS/GB
  fsType: ext4
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true

---
# 2. Local PersistentVolume ã‚’ä½¿ç”¨ï¼ˆæœ€é«˜é€Ÿï¼‰
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-1
spec:
  capacity:
    storage: 100Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage
  local:
    path: /mnt/disks/ssd1
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node-1

---
# 3. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ´»ç”¨
# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å±¤ã§ã®ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ï¼ˆRedisç­‰ï¼‰
# ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ„è­˜ã—ãŸå®Ÿè£…
```

---

## 4. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å•é¡Œ

### 4.1 DNS Resolution Failures

#### è¨ºæ–­

```bash
# Podå†…ã‹ã‚‰DNSè§£æ±ºãƒ†ã‚¹ãƒˆ
kubectl exec -it myapp-7d9f8b4c5d-xyz -- nslookup kubernetes.default

# å¤±æ•—ã™ã‚‹å ´åˆ:
# Server:         10.96.0.10
# Address:        10.96.0.10:53
#
# ** server can't find kubernetes.default: NXDOMAIN

# CoreDNS Pod ã®çŠ¶æ…‹ç¢ºèª
kubectl get pods -n kube-system -l k8s-app=kube-dns

# CoreDNS ãƒ­ã‚°ç¢ºèª
kubectl logs -n kube-system -l k8s-app=kube-dns

# DNSè¨­å®šç¢ºèª
kubectl exec -it myapp-7d9f8b4c5d-xyz -- cat /etc/resolv.conf

# å‡ºåŠ›:
# search default.svc.cluster.local svc.cluster.local cluster.local
# nameserver 10.96.0.10
# options ndots:5
```

#### å¯¾å‡¦æ³•

```yaml
# 1. CoreDNS ConfigMap ç¢ºèª
kubectl get cm -n kube-system coredns -o yaml

# ä¾‹: ã‚«ã‚¹ã‚¿ãƒ ãƒ‰ãƒ¡ã‚¤ãƒ³ã®è¿½åŠ 
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health {
          lameduck 5s
        }
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
          pods insecure
          fallthrough in-addr.arpa ip6.arpa
          ttl 30
        }
        prometheus :9153
        forward . /etc/resolv.conf {
          max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance
    }
    # ã‚«ã‚¹ã‚¿ãƒ ãƒ‰ãƒ¡ã‚¤ãƒ³
    example.com:53 {
        errors
        cache 30
        forward . 8.8.8.8 8.8.4.4
    }

---
# 2. NodeLocal DNSCache ã‚’å°å…¥ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ï¼‰
kubectl apply -f https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml

# NodeLocal DNSCache DaemonSet ã®ç¢ºèª
kubectl get ds -n kube-system node-local-dns

---
# 3. Pod ã® DNSConfig ã‚’ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
apiVersion: v1
kind: Pod
metadata:
  name: myapp
spec:
  containers:
  - name: app
    image: myapp:1.0
  dnsPolicy: "None"
  dnsConfig:
    nameservers:
    - 10.96.0.10       # CoreDNS
    - 8.8.8.8          # Google DNS (fallback)
    searches:
    - default.svc.cluster.local
    - svc.cluster.local
    - cluster.local
    options:
    - name: ndots
      value: "2"
    - name: edns0
```

### 4.2 Service Connectivity Issues

```bash
# Service ã®ç¢ºèª
kubectl get svc myapp

# Endpoint ã®ç¢ºèªï¼ˆå®Ÿéš›ã®Pod IPãŒç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã‹ï¼‰
kubectl get endpoints myapp

# å‡ºåŠ›ä¾‹:
# NAME    ENDPOINTS                                               AGE
# myapp   10.244.1.10:8080,10.244.2.15:8080,10.244.3.20:8080     10d

# Endpoint ãŒç©ºã®å ´åˆã€Serviceã®selectorã‚’ç¢ºèª
kubectl get svc myapp -o yaml | grep -A 5 selector
kubectl get pods -l app=myapp  # selectorã«ãƒãƒƒãƒã™ã‚‹PodãŒå­˜åœ¨ã™ã‚‹ã‹

# ServiceçµŒç”±ã§ã®æ¥ç¶šãƒ†ã‚¹ãƒˆ
kubectl run test-pod --image=curlimages/curl -it --rm -- curl http://myapp.default.svc.cluster.local:8080

# ç›´æ¥PodIPã¸ã®æ¥ç¶šãƒ†ã‚¹ãƒˆ
kubectl run test-pod --image=curlimages/curl -it --rm -- curl http://10.244.1.10:8080

# Serviceã¯ç–é€šã™ã‚‹ãŒPod IPã¯ç–é€šã—ãªã„ â†’ kube-proxy ã®å•é¡Œ
# Pod IPã¯ç–é€šã™ã‚‹ãŒServiceã¯ç–é€šã—ãªã„ â†’ Serviceå®šç¾©ã®å•é¡Œ
```

#### kube-proxy ã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

```bash
# kube-proxy Podã®ç¢ºèª
kubectl get pods -n kube-system -l k8s-app=kube-proxy

# kube-proxy ãƒ­ã‚°ç¢ºèª
kubectl logs -n kube-system -l k8s-app=kube-proxy

# kube-proxy ã® mode ç¢ºèª
kubectl logs -n kube-system kube-proxy-xxxxx | grep "Using"
# å‡ºåŠ›: Using iptables Proxy

# iptables ãƒ«ãƒ¼ãƒ«ã®ç¢ºèªï¼ˆNodeä¸Šï¼‰
kubectl debug node/node-1 -it --image=ubuntu:22.04
chroot /host

# Service ã® iptables ãƒ«ãƒ¼ãƒ«ç¢ºèª
iptables-save | grep myapp

# å‡ºåŠ›ä¾‹:
# -A KUBE-SERVICES -d 10.96.100.200/32 -p tcp -m tcp --dport 8080 -j KUBE-SVC-XXXXXX
# -A KUBE-SVC-XXXXXX -m statistic --mode random --probability 0.33333 -j KUBE-SEP-AAAA
# -A KUBE-SVC-XXXXXX -m statistic --mode random --probability 0.50000 -j KUBE-SEP-BBBB
# -A KUBE-SVC-XXXXXX -j KUBE-SEP-CCCC
```

### 4.3 Network Policy Issues

```bash
# Network Policy ã®ç¢ºèª
kubectl get networkpolicies -n prod

# ç‰¹å®šPodã«é©ç”¨ã•ã‚Œã¦ã„ã‚‹Network Policyã‚’ç¢ºèª
kubectl describe pod myapp-7d9f8b4c5d-xyz | grep -A 10 "Labels"

# Network Policy ã®è©³ç´°
kubectl describe networkpolicy allow-frontend -n prod

# Network Policyã®ãƒ†ã‚¹ãƒˆ
# é€ä¿¡å…ƒPod
kubectl run test-source --image=curlimages/curl -n prod -- sleep 3600

# å®›å…ˆPodã¸ã®æ¥ç¶šãƒ†ã‚¹ãƒˆ
kubectl exec -n prod test-source -- curl -m 5 http://myapp:8080

# ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã™ã‚‹å ´åˆã€Network Policyã§æ‹’å¦ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§
```

#### Network Policy ãƒ‡ãƒãƒƒã‚°ä¾‹

```yaml
# å•é¡Œ: frontendã‹ã‚‰backendã«æ¥ç¶šã§ããªã„

# ç¾åœ¨ã®Network Policy
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend-allow
  namespace: prod
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
          tier: web  # â† å•é¡Œ: frontendã«tier=webãƒ©ãƒ™ãƒ«ãŒãªã„
    ports:
    - protocol: TCP
      port: 8080

# frontendã®ãƒ©ãƒ™ãƒ«ç¢ºèª
kubectl get pod -n prod -l app=frontend --show-labels
# NAME                        READY   STATUS    LABELS
# frontend-7d9f8b4c5d-xyz     1/1     Running   app=frontend

# ä¿®æ­£æ¡ˆ1: tier=web ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ 
kubectl label pod -n prod -l app=frontend tier=web

# ä¿®æ­£æ¡ˆ2: Network Policyã‚’ä¿®æ­£ï¼ˆtierãƒ©ãƒ™ãƒ«ã‚’å‰Šé™¤ï¼‰
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend-allow
  namespace: prod
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend  # tier: webã‚’å‰Šé™¤
    ports:
    - protocol: TCP
      port: 8080
```

### 4.4 Ingress Issues

```bash
# Ingress ã®ç¢ºèª
kubectl get ingress -n prod

# Ingress è©³ç´°
kubectl describe ingress myapp-ingress -n prod

# Events:
#   Type    Reason  Age   From                      Message
#   ----    ------  ----  ----                      -------
#   Normal  Sync    1m    nginx-ingress-controller  Scheduled for sync

# Ingress Controller Pod ã®ç¢ºèª
kubectl get pods -n ingress-nginx

# Ingress Controller ãƒ­ã‚°
kubectl logs -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx

# Backend Service ã®ç–é€šç¢ºèª
kubectl run test --image=curlimages/curl -it --rm -- curl http://myapp.prod.svc.cluster.local:8080

# TLS è¨¼æ˜æ›¸ã®ç¢ºèª
kubectl get secret -n prod myapp-tls -o yaml

# è¨¼æ˜æ›¸ã®æœ‰åŠ¹æœŸé™ç¢ºèª
kubectl get secret -n prod myapp-tls -o jsonpath='{.data.tls\.crt}' | base64 -d | openssl x509 -noout -dates
```

---

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€Kubernetesæœ¬ç•ªç’°å¢ƒã§ã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®ä¸»è¦ãªå•é¡Œã¨ãã®è§£æ±ºæ–¹æ³•ã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã¾ã™ã€‚

## ğŸ“š å‚è€ƒãƒªã‚½ãƒ¼ã‚¹

- [Kubernetes Debugging Pods](https://kubernetes.io/docs/tasks/debug/debug-application/debug-pods/)
- [kubectl debug Documentation](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#debug)
- [Troubleshooting Applications](https://kubernetes.io/docs/tasks/debug/debug-application/)
- [Troubleshooting Clusters](https://kubernetes.io/docs/tasks/debug/debug-cluster/)
- [Network Policy Recipes](https://github.com/ahmetb/kubernetes-network-policy-recipes)
