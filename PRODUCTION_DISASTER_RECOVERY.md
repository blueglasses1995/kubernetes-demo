# Kubernetesæœ¬ç•ªç’°å¢ƒ ç½å®³å¾©æ—§ï¼ˆDRï¼‰å®Œå…¨ã‚¬ã‚¤ãƒ‰

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€Kubernetesæœ¬ç•ªç’°å¢ƒã§å¿…é ˆã¨ãªã‚‹ç½å®³å¾©æ—§ï¼ˆDisaster Recoveryï¼‰ã®å®Ÿè£…ã‚’ã€æŠ€è¡“çš„åŸç†ã‹ã‚‰å®Ÿè·µã¾ã§å¾¹åº•çš„ã«è§£èª¬ã—ã¾ã™ã€‚

## ğŸ“š ç›®æ¬¡

1. [etcd ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—](#1-etcd-ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—)
2. [Veleroã«ã‚ˆã‚‹ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—](#2-veleroã«ã‚ˆã‚‹ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—)
3. [RTO/RPO ã®è¨­è¨ˆ](#3-rtorpo-ã®è¨­è¨ˆ)
4. [Multi-cluster DR](#4-multi-cluster-dr)
5. [DRè¨“ç·´ã®å®Ÿæ–½](#5-drè¨“ç·´ã®å®Ÿæ–½)

---

## 1. etcd ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

### 1.1 etcd ã®æŠ€è¡“çš„é‡è¦æ€§

#### etcd ãŒä¿å­˜ã™ã‚‹é‡è¦ãƒ‡ãƒ¼ã‚¿

```
etcd ã¯ Kubernetes ã®ã™ã¹ã¦ã®çŠ¶æ…‹ã‚’ä¿å­˜:

/registry/
â”œâ”€â”€ pods/                           # ã™ã¹ã¦ã®Podå®šç¾©
â”œâ”€â”€ services/                       # ã™ã¹ã¦ã®Service
â”œâ”€â”€ deployments/                    # Deploymentã€ReplicaSet
â”œâ”€â”€ configmaps/                     # ConfigMap
â”œâ”€â”€ secrets/                        # Secretsï¼ˆæš—å·åŒ–ã•ã‚Œã¦ã„ã‚‹å ´åˆã‚‚ï¼‰
â”œâ”€â”€ rbac.authorization.k8s.io/      # RBACè¨­å®š
â”œâ”€â”€ namespaces/                     # Namespaceå®šç¾©
â”œâ”€â”€ persistentvolumes/              # PV/PVC
â”œâ”€â”€ customresourcedefinitions/      # CRDå®šç¾©
â””â”€â”€ <custom resources>/             # ã™ã¹ã¦ã®ã‚«ã‚¹ã‚¿ãƒ ãƒªã‚½ãƒ¼ã‚¹

âš ï¸ etcdãŒå¤±ã‚ã‚Œã‚‹ã¨ã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å…¨ä½“ãŒå¾©æ—§ä¸å¯èƒ½ âš ï¸
```

### 1.2 etcdctl ã‚’ä½¿ã£ãŸæ‰‹å‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

#### ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚³ãƒãƒ³ãƒ‰

```bash
# etcd Podå†…ã§å®Ÿè¡Œï¼ˆé™çš„Pod: /etc/kubernetes/manifests/etcd.yamlï¼‰
ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /backup/etcd-snapshot-$(date +%Y%m%d-%H%M%S).db

# å‡ºåŠ›ä¾‹:
# {"level":"info","ts":1705300800.123,"caller":"snapshot/v3_snapshot.go:68","msg":"created temporary db file","path":"/backup/etcd-snapshot-20240115-103000.db.part"}
# {"level":"info","ts":1705300800.456,"msg":"fetching snapshot","endpoint":"https://127.0.0.1:2379"}
# {"level":"info","ts":1705300801.789,"msg":"fetched snapshot","endpoint":"https://127.0.0.1:2379","size":"64 MB","took":"now"}
# Snapshot saved at /backup/etcd-snapshot-20240115-103000.db

# ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®æ¤œè¨¼
ETCDCTL_API=3 etcdctl \
  --write-out=table \
  snapshot status /backup/etcd-snapshot-20240115-103000.db

# å‡ºåŠ›:
# +----------+----------+------------+------------+
# |   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
# +----------+----------+------------+------------+
# | 12345678 |   123456 |      15234 |     64 MB  |
# +----------+----------+------------+------------+
```

### 1.3 è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å®Ÿè£…ï¼ˆCronJobï¼‰

#### etcd ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— CronJob

```yaml
---
# 1. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç”¨ ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: etcd-backup
  namespace: kube-system

---
# 2. RBACè¨­å®šï¼ˆetcd Podã¸ã®execæ¨©é™ï¼‰
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: etcd-backup
  namespace: kube-system
rules:
- apiGroups: [""]
  resources: ["pods", "pods/exec"]
  verbs: ["get", "list", "create"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: etcd-backup
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: etcd-backup
subjects:
- kind: ServiceAccount
  name: etcd-backup
  namespace: kube-system

---
# 3. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: etcd-backup-script
  namespace: kube-system
data:
  backup.sh: |
    #!/bin/bash
    set -euo pipefail

    # å¤‰æ•°è¨­å®š
    TIMESTAMP=$(date +%Y%m%d-%H%M%S)
    BACKUP_FILE="etcd-snapshot-${TIMESTAMP}.db"
    S3_BUCKET="s3://my-k8s-backups/etcd"
    RETENTION_DAYS=30

    echo "Starting etcd backup at ${TIMESTAMP}"

    # etcd Podã®æ¤œå‡º
    ETCD_POD=$(kubectl get pods -n kube-system -l component=etcd -o jsonpath='{.items[0].metadata.name}')
    echo "etcd Pod: ${ETCD_POD}"

    # etcd ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä½œæˆ
    kubectl exec -n kube-system ${ETCD_POD} -- sh -c \
      "ETCDCTL_API=3 etcdctl \
        --endpoints=https://127.0.0.1:2379 \
        --cacert=/etc/kubernetes/pki/etcd/ca.crt \
        --cert=/etc/kubernetes/pki/etcd/server.crt \
        --key=/etc/kubernetes/pki/etcd/server.key \
        snapshot save /tmp/${BACKUP_FILE}"

    # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®ã‚³ãƒ”ãƒ¼
    kubectl cp -n kube-system ${ETCD_POD}:/tmp/${BACKUP_FILE} /tmp/${BACKUP_FILE}

    # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®æ¤œè¨¼
    ETCDCTL_API=3 etcdctl snapshot status /tmp/${BACKUP_FILE} --write-out=json > /tmp/status.json
    REVISION=$(cat /tmp/status.json | jq -r '.revision')
    TOTAL_KEYS=$(cat /tmp/status.json | jq -r '.totalKey')

    echo "Snapshot created: revision=${REVISION}, keys=${TOTAL_KEYS}"

    # S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    aws s3 cp /tmp/${BACKUP_FILE} ${S3_BUCKET}/${BACKUP_FILE} \
      --metadata "revision=${REVISION},total-keys=${TOTAL_KEYS},timestamp=${TIMESTAMP}"

    echo "Uploaded to ${S3_BUCKET}/${BACKUP_FILE}"

    # å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å‰Šé™¤
    echo "Cleaning up old backups (retention: ${RETENTION_DAYS} days)"
    aws s3 ls ${S3_BUCKET}/ | while read -r line; do
      createDate=$(echo $line | awk '{print $1" "$2}')
      createDate=$(date -d "$createDate" +%s)
      olderThan=$(date -d "-${RETENTION_DAYS} days" +%s)
      if [[ $createDate -lt $olderThan ]]; then
        fileName=$(echo $line | awk '{print $4}')
        if [[ $fileName != "" ]]; then
          aws s3 rm ${S3_BUCKET}/${fileName}
          echo "Deleted: ${fileName}"
        fi
      fi
    done

    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
    rm -f /tmp/${BACKUP_FILE} /tmp/status.json

    echo "Backup completed successfully"

---
# 4. CronJobå®šç¾©
apiVersion: batch/v1
kind: CronJob
metadata:
  name: etcd-backup
  namespace: kube-system
spec:
  # æ¯æ—¥åˆå‰3æ™‚ã«å®Ÿè¡Œ
  schedule: "0 3 * * *"

  # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³è¨­å®šï¼ˆKubernetes 1.25+ï¼‰
  timeZone: "Asia/Tokyo"

  # æˆåŠŸã—ãŸJobã‚’3å€‹ä¿æŒ
  successfulJobsHistoryLimit: 3

  # å¤±æ•—ã—ãŸJobã‚’1å€‹ä¿æŒ
  failedJobsHistoryLimit: 1

  # ä¸¦è¡Œå®Ÿè¡Œã‚’ç¦æ­¢
  concurrencyPolicy: Forbid

  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: etcd-backup
        spec:
          serviceAccountName: etcd-backup
          restartPolicy: OnFailure

          containers:
          - name: backup
            image: amazon/aws-cli:2.13.0
            command:
            - /bin/bash
            - /scripts/backup.sh

            env:
            # AWSèªè¨¼ï¼ˆIRSAã‚’ä½¿ç”¨ï¼‰
            - name: AWS_REGION
              value: ap-northeast-1
            - name: AWS_ROLE_ARN
              value: arn:aws:iam::123456789012:role/EtcdBackupRole
            - name: AWS_WEB_IDENTITY_TOKEN_FILE
              value: /var/run/secrets/eks.amazonaws.com/serviceaccount/token

            volumeMounts:
            - name: scripts
              mountPath: /scripts
            - name: aws-token
              mountPath: /var/run/secrets/eks.amazonaws.com/serviceaccount
              readOnly: true

            resources:
              requests:
                cpu: 100m
                memory: 128Mi
              limits:
                cpu: 500m
                memory: 512Mi

          volumes:
          - name: scripts
            configMap:
              name: etcd-backup-script
              defaultMode: 0755
          - name: aws-token
            projected:
              sources:
              - serviceAccountToken:
                  path: token
                  expirationSeconds: 86400
                  audience: sts.amazonaws.com
```

### 1.4 etcd ãƒªã‚¹ãƒˆã‚¢ï¼ˆå¾©æ—§ï¼‰æ‰‹é †

#### ãƒªã‚¹ãƒˆã‚¢å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
#!/bin/bash
# etcd-restore.sh - etcd ã‚’å®Œå…¨ã«å¾©æ—§ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

set -euo pipefail

BACKUP_FILE="$1"
ETCD_DATA_DIR="/var/lib/etcd"

echo "=== etcd Restore Procedure ==="
echo "Backup file: ${BACKUP_FILE}"
echo "âš ï¸  WARNING: This will COMPLETELY REPLACE the current etcd data âš ï¸"
read -p "Are you sure you want to continue? (type 'yes' to continue): " confirm

if [ "$confirm" != "yes" ]; then
  echo "Restore cancelled"
  exit 1
fi

# 1. ã™ã¹ã¦ã®kube-apiserver ã‚’åœæ­¢
echo "Step 1: Stopping kube-apiserver..."
sudo mv /etc/kubernetes/manifests/kube-apiserver.yaml /tmp/
sleep 10

# kube-apiserverãŒåœæ­¢ã—ãŸã“ã¨ã‚’ç¢ºèª
while pgrep -x "kube-apiserver" > /dev/null; do
  echo "Waiting for kube-apiserver to stop..."
  sleep 2
done
echo "kube-apiserver stopped"

# 2. etcd ã‚’åœæ­¢
echo "Step 2: Stopping etcd..."
sudo mv /etc/kubernetes/manifests/etcd.yaml /tmp/
sleep 10

# etcdãŒåœæ­¢ã—ãŸã“ã¨ã‚’ç¢ºèª
while pgrep -x "etcd" > /dev/null; do
  echo "Waiting for etcd to stop..."
  sleep 2
done
echo "etcd stopped"

# 3. æ—¢å­˜ã®etcdãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆå¿µã®ãŸã‚ï¼‰
echo "Step 3: Backing up current etcd data..."
sudo mv ${ETCD_DATA_DIR} ${ETCD_DATA_DIR}.backup.$(date +%Y%m%d-%H%M%S)

# 4. ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‹ã‚‰ãƒªã‚¹ãƒˆã‚¢
echo "Step 4: Restoring from snapshot..."
sudo ETCDCTL_API=3 etcdctl snapshot restore ${BACKUP_FILE} \
  --data-dir=${ETCD_DATA_DIR} \
  --name=master-1 \
  --initial-cluster=master-1=https://192.168.1.10:2380 \
  --initial-cluster-token=etcd-cluster-1 \
  --initial-advertise-peer-urls=https://192.168.1.10:2380

# æ¨©é™è¨­å®š
sudo chown -R etcd:etcd ${ETCD_DATA_DIR}

echo "Snapshot restored to ${ETCD_DATA_DIR}"

# 5. etcd ã‚’èµ·å‹•
echo "Step 5: Starting etcd..."
sudo mv /tmp/etcd.yaml /etc/kubernetes/manifests/
sleep 10

# etcdãŒèµ·å‹•ã—ãŸã“ã¨ã‚’ç¢ºèª
until kubectl --kubeconfig=/etc/kubernetes/admin.conf get cs 2>/dev/null | grep -q etcd; do
  echo "Waiting for etcd to start..."
  sleep 2
done
echo "etcd started"

# 6. kube-apiserver ã‚’èµ·å‹•
echo "Step 6: Starting kube-apiserver..."
sudo mv /tmp/kube-apiserver.yaml /etc/kubernetes/manifests/
sleep 10

# kube-apiserverãŒèµ·å‹•ã—ãŸã“ã¨ã‚’ç¢ºèª
until kubectl get nodes 2>/dev/null; do
  echo "Waiting for kube-apiserver to start..."
  sleep 2
done
echo "kube-apiserver started"

# 7. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®çŠ¶æ…‹ç¢ºèª
echo "Step 7: Verifying cluster status..."
kubectl get nodes
kubectl get pods --all-namespaces

echo ""
echo "=== Restore completed successfully ==="
echo "Please verify that all resources are restored correctly."
```

---

## 2. Veleroã«ã‚ˆã‚‹ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

### 2.1 Velero ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kubernetes Cluster                                         â”‚
â”‚                                                             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚ â”‚ Velero Server (Deployment)                          â”‚    â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚
â”‚ â”‚ â”‚ Controllers:                                    â”‚ â”‚    â”‚
â”‚ â”‚ â”‚ ãƒ»Backup Controller                             â”‚ â”‚    â”‚
â”‚ â”‚ â”‚ ãƒ»Restore Controller                            â”‚ â”‚    â”‚
â”‚ â”‚ â”‚ ãƒ»Schedule Controller                           â”‚ â”‚    â”‚
â”‚ â”‚ â”‚ ãƒ»Download Request Controller                   â”‚ â”‚    â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                          â”‚                                  â”‚
â”‚                          â”‚ kubectl API                      â”‚
â”‚                          â–¼                                  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚ â”‚ Kubernetes API Server                               â”‚    â”‚
â”‚ â”‚ ãƒ»Namespaces                                        â”‚    â”‚
â”‚ â”‚ ãƒ»Deployments, StatefulSets, etc.                   â”‚    â”‚
â”‚ â”‚ ãƒ»ConfigMaps, Secrets                               â”‚    â”‚
â”‚ â”‚ ãƒ»PersistentVolumeClaims                            â”‚    â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                          â”‚                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Object Storage (S3 / GCS / Azure Blob)                       â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚ â”‚ backups/                                             â”‚     â”‚
â”‚ â”‚ â”œâ”€ backup-20240115-030000/                           â”‚     â”‚
â”‚ â”‚ â”‚  â”œâ”€ backup.tar.gz        â† ãƒªã‚½ãƒ¼ã‚¹å®šç¾©           â”‚     â”‚
â”‚ â”‚ â”‚  â”œâ”€ backup-logs.gz       â† ãƒ­ã‚°                   â”‚     â”‚
â”‚ â”‚ â”‚  â”œâ”€ backup-resources.json                          â”‚     â”‚
â”‚ â”‚ â”‚  â””â”€ backup-volumesnapshots.json                    â”‚     â”‚
â”‚ â”‚ â””â”€ backup-20240116-030000/                           â”‚     â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Volume Snapshots (CSI Snapshots)                             â”‚
â”‚ â”œâ”€ EBS Snapshots (AWS)                                       â”‚
â”‚ â”œâ”€ Persistent Disk Snapshots (GCP)                           â”‚
â”‚ â””â”€ Disk Snapshots (Azure)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Velero ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# 1. Velero CLI ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# macOS
brew install velero

# Linux
wget https://github.com/vmware-tanzu/velero/releases/download/v1.12.0/velero-v1.12.0-linux-amd64.tar.gz
tar -xvf velero-v1.12.0-linux-amd64.tar.gz
sudo mv velero-v1.12.0-linux-amd64/velero /usr/local/bin/

# 2. AWSç”¨ã®è¨­å®šï¼ˆS3ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼‰
cat > credentials-velero <<EOF
[default]
aws_access_key_id = <ACCESS_KEY_ID>
aws_secret_access_key = <SECRET_ACCESS_KEY>
EOF

# 3. Velero ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆAWSï¼‰
velero install \
  --provider aws \
  --plugins velero/velero-plugin-for-aws:v1.8.0 \
  --bucket my-velero-backups \
  --backup-location-config region=ap-northeast-1 \
  --snapshot-location-config region=ap-northeast-1 \
  --secret-file ./credentials-velero \
  --use-volume-snapshots=true \
  --use-node-agent \
  --uploader-type=restic

# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
kubectl get pods -n velero

# å‡ºåŠ›:
# NAME                      READY   STATUS    RESTARTS   AGE
# velero-7d9f8b4c5d-xyz     1/1     Running   0          2m
# node-agent-abc123         1/1     Running   0          2m
# node-agent-def456         1/1     Running   0          2m
```

### 2.3 ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ä½œæˆ

#### åŸºæœ¬çš„ãªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

```bash
# 1. ç‰¹å®šNamespaceã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
velero backup create prod-backup \
  --include-namespaces prod \
  --wait

# 2. ç‰¹å®šãƒ©ãƒ™ãƒ«ã‚’æŒã¤ãƒªã‚½ãƒ¼ã‚¹ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
velero backup create app-backup \
  --selector app=myapp

# 3. ã™ã¹ã¦ã®Namespaceã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆkube-systemé™¤ãï¼‰
velero backup create cluster-backup \
  --exclude-namespaces kube-system,kube-public,kube-node-lease,velero

# 4. PersistentVolumeã‚’å«ã‚€ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
velero backup create fullbackup \
  --include-namespaces prod \
  --snapshot-volumes=true \
  --default-volumes-to-fs-backup  # Resticã‚’ä½¿ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®ç¢ºèª
velero backup describe prod-backup --details

# å‡ºåŠ›ä¾‹:
# Name:         prod-backup
# Namespace:    velero
# Labels:       velero.io/storage-location=default
# Annotations:  <none>
#
# Phase:  Completed
#
# Namespaces:
#   Included:  prod
#   Excluded:  <none>
#
# Resources:
#   Included:        *
#   Excluded:        <none>
#   Cluster-scoped:  auto
#
# Backup Format Version:  1.1.0
#
# Started:    2024-01-15 03:00:00 +0900 JST
# Completed:  2024-01-15 03:02:30 +0900 JST
#
# Expiration:  2024-02-14 03:00:00 +0900 JST
#
# Total items to be backed up:  150
# Items backed up:              150
#
# Resource List:
#   apps/v1/Deployment:
#     - prod/myapp
#     - prod/nginx
#   v1/Service:
#     - prod/myapp
#     - prod/nginx
#   v1/ConfigMap:
#     - prod/myapp-config
#   v1/Secret:
#     - prod/myapp-secrets
#   v1/PersistentVolumeClaim:
#     - prod/myapp-data
```

#### ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

```yaml
# Schedule CRD ã§ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: daily-backup
  namespace: velero
spec:
  # cronå½¢å¼ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
  schedule: "0 3 * * *"  # æ¯æ—¥åˆå‰3æ™‚

  # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
  template:
    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¯¾è±¡
    includedNamespaces:
    - prod
    - staging

    # é™¤å¤–ã™ã‚‹ãƒªã‚½ãƒ¼ã‚¹
    excludedResources:
    - events
    - events.events.k8s.io

    # ãƒœãƒªãƒ¥ãƒ¼ãƒ ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
    snapshotVolumes: true
    defaultVolumesToFsBackup: true

    # TTLï¼ˆä¿æŒæœŸé–“ï¼‰
    ttl: 720h  # 30æ—¥é–“

    # ãƒ©ãƒ™ãƒ«
    labelSelector:
      matchExpressions:
      - key: backup
        operator: NotIn
        values:
        - "false"

    # ãƒ•ãƒƒã‚¯ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‰å¾Œã®å‡¦ç†ï¼‰
    hooks:
      resources:
      - name: mysql-backup-hook
        includedNamespaces:
        - prod
        labelSelector:
          matchLabels:
            app: mysql
        pre:
        - exec:
            container: mysql
            command:
            - /bin/bash
            - -c
            - "mysqldump -u root -p$MYSQL_ROOT_PASSWORD --all-databases > /backup/dump.sql"
            onError: Fail
            timeout: 10m
        post:
        - exec:
            container: mysql
            command:
            - /bin/bash
            - -c
            - "rm -f /backup/dump.sql"
```

```bash
# CLIã§ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ä½œæˆ
velero schedule create daily-backup \
  --schedule="0 3 * * *" \
  --include-namespaces prod,staging \
  --ttl 720h

# ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ç¢ºèª
velero schedule get

# å‡ºåŠ›:
# NAME           STATUS    CREATED                         SCHEDULE    BACKUP TTL   LAST BACKUP   SELECTOR
# daily-backup   Enabled   2024-01-15 10:00:00 +0900 JST   0 3 * * *   720h0m0s     2h ago        <none>

# æ‰‹å‹•ã§ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œ
velero backup create --from-schedule daily-backup
```

### 2.4 ãƒªã‚¹ãƒˆã‚¢ï¼ˆå¾©æ—§ï¼‰

```bash
# 1. æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ã®ãƒªã‚¹ãƒˆã‚¢
velero restore create --from-backup prod-backup

# 2. ç‰¹å®šNamespaceã®ã¿ãƒªã‚¹ãƒˆã‚¢
velero restore create prod-restore \
  --from-backup cluster-backup \
  --include-namespaces prod

# 3. ç‰¹å®šãƒªã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã®ã¿ãƒªã‚¹ãƒˆã‚¢
velero restore create config-restore \
  --from-backup prod-backup \
  --include-resources configmaps,secrets

# 4. æ—¢å­˜ãƒªã‚½ãƒ¼ã‚¹ã‚’ç½®ãæ›ãˆãªã„ãƒªã‚¹ãƒˆã‚¢
velero restore create careful-restore \
  --from-backup prod-backup \
  --existing-resource-policy=none  # æ—¢å­˜ãƒªã‚½ãƒ¼ã‚¹ã¯ã‚¹ã‚­ãƒƒãƒ—

# 5. Namespace ã‚’å¤‰æ›´ã—ã¦ãƒªã‚¹ãƒˆã‚¢ï¼ˆæœ¬ç•ªâ†’æ¤œè¨¼ç’°å¢ƒï¼‰
velero restore create test-restore \
  --from-backup prod-backup \
  --namespace-mappings prod:test

# ãƒªã‚¹ãƒˆã‚¢ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®ç¢ºèª
velero restore describe prod-restore

# å‡ºåŠ›:
# Name:         prod-restore
# Namespace:    velero
# Labels:       <none>
# Annotations:  <none>
#
# Phase:                       Completed
# Backup:                      prod-backup
# Namespaces:
#   Included:  prod
#   Excluded:  <none>
#
# Resources:
#   Included:        *
#   Excluded:        nodes, events, events.events.k8s.io
#   Cluster-scoped:  auto
#
# Namespace mappings:  <none>
#
# Restore PVs:  auto
#
# Started:    2024-01-15 15:00:00 +0900 JST
# Completed:  2024-01-15 15:05:30 +0900 JST
#
# Warnings:  0
# Errors:    0
```

---

## 3. RTO/RPO ã®è¨­è¨ˆ

### 3.1 RTO ã¨ RPO ã®å®šç¾©

```
Timeline of a Disaster:

æ­£å¸¸é‹ç”¨          éšœå®³ç™ºç”Ÿ                      å¾©æ—§å®Œäº†
   â”‚                â”‚                              â”‚
   â–¼                â–¼                              â–¼
â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€
                    â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ RTO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º

                    â—„â”€â”€ RPO â”€â”€â–º
                    â”‚          â”‚
                 æœ€å¾Œã®      éšœå®³
                ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—  ç™ºç”Ÿ

RPO (Recovery Point Objective):
  ãƒ»è¨±å®¹å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿æå¤±ã®æ™‚é–“
  ãƒ»ã€Œã©ã“ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ã‚’å¤±ã£ã¦ã‚‚è¨±å®¹ã§ãã‚‹ã‹ã€
  ãƒ»ä¾‹: RPO = 1æ™‚é–“ â†’ æœ€å¤§1æ™‚é–“åˆ†ã®ãƒ‡ãƒ¼ã‚¿æå¤±ã‚’è¨±å®¹

RTO (Recovery Time Objective):
  ãƒ»ç›®æ¨™å¾©æ—§æ™‚é–“
  ãƒ»ã€Œã©ã‚Œãã‚‰ã„ã®æ™‚é–“ã§ã‚µãƒ¼ãƒ“ã‚¹ã‚’å¾©æ—§ã•ã›ã‚‹ã‹ã€
  ãƒ»ä¾‹: RTO = 4æ™‚é–“ â†’ 4æ™‚é–“ä»¥å†…ã«ã‚µãƒ¼ãƒ“ã‚¹å¾©æ—§
```

### 3.2 ã‚µãƒ¼ãƒ“ã‚¹ãƒ¬ãƒ™ãƒ«ã”ã¨ã®RTO/RPOè¨­è¨ˆ

#### Tier 1: ãƒŸãƒƒã‚·ãƒ§ãƒ³ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«

```yaml
# Tier 1: æ±ºæ¸ˆã‚·ã‚¹ãƒ†ãƒ ã€èªè¨¼ã‚·ã‚¹ãƒ†ãƒ ç­‰
# RTO: 1æ™‚é–“ä»¥å†…
# RPO: 5åˆ†ä»¥å†…

---
# 1. etcd ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: 10åˆ†ã”ã¨
apiVersion: batch/v1
kind: CronJob
metadata:
  name: etcd-backup-tier1
spec:
  schedule: "*/10 * * * *"  # 10åˆ†ã”ã¨
  # ... (å‰è¿°ã®etcdãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨­å®š)

---
# 2. Velero ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: 1æ™‚é–“ã”ã¨
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: tier1-hourly
spec:
  schedule: "0 * * * *"  # æ¯æ™‚0åˆ†
  template:
    includedNamespaces:
    - payment
    - auth
    labelSelector:
      matchLabels:
        tier: "1"
    snapshotVolumes: true
    ttl: 168h  # 7æ—¥é–“ä¿æŒ

---
# 3. Multi-region ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
# - ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’è¤‡æ•°ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã«é…ç½®
# - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯ãƒãƒ«ãƒãƒªãƒ¼ã‚¸ãƒ§ãƒ³ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
# - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åŒæœŸ
```

#### Tier 2: é‡è¦ã‚·ã‚¹ãƒ†ãƒ 

```yaml
# Tier 2: APIã‚µãƒ¼ãƒãƒ¼ã€ç®¡ç†ç”»é¢ç­‰
# RTO: 4æ™‚é–“ä»¥å†…
# RPO: 1æ™‚é–“ä»¥å†…

---
# 1. etcd ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: 1æ™‚é–“ã”ã¨
apiVersion: batch/v1
kind: CronJob
metadata:
  name: etcd-backup-tier2
spec:
  schedule: "0 * * * *"

---
# 2. Velero ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: 6æ™‚é–“ã”ã¨
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: tier2-6hourly
spec:
  schedule: "0 */6 * * *"  # 6æ™‚é–“ã”ã¨
  template:
    includedNamespaces:
    - api
    - admin
    labelSelector:
      matchLabels:
        tier: "2"
    ttl: 720h  # 30æ—¥é–“ä¿æŒ
```

#### Tier 3: é€šå¸¸ã‚·ã‚¹ãƒ†ãƒ 

```yaml
# Tier 3: å†…éƒ¨ãƒ„ãƒ¼ãƒ«ã€é–‹ç™ºç’°å¢ƒç­‰
# RTO: 24æ™‚é–“ä»¥å†…
# RPO: 24æ™‚é–“ä»¥å†…

---
# 1. etcd ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: 1æ—¥1å›
apiVersion: batch/v1
kind: CronJob
metadata:
  name: etcd-backup-tier3
spec:
  schedule: "0 3 * * *"  # æ¯æ—¥åˆå‰3æ™‚

---
# 2. Velero ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: 1æ—¥1å›
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: tier3-daily
spec:
  schedule: "0 3 * * *"
  template:
    includedNamespaces:
    - tools
    - dev
    labelSelector:
      matchLabels:
        tier: "3"
    ttl: 2160h  # 90æ—¥é–“ä¿æŒ
```

### 3.3 ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¿æŒãƒãƒªã‚·ãƒ¼

```
ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸–ä»£ç®¡ç†:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Grandfather-Father-Son (GFS) æ–¹å¼                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Daily (Son):
â”œâ”€ backup-20240115-030000 (ä»Šæ—¥)
â”œâ”€ backup-20240114-030000 (1æ—¥å‰)
â”œâ”€ backup-20240113-030000 (2æ—¥å‰)
â”œâ”€ backup-20240112-030000 (3æ—¥å‰)
â”œâ”€ backup-20240111-030000 (4æ—¥å‰)
â”œâ”€ backup-20240110-030000 (5æ—¥å‰)
â””â”€ backup-20240109-030000 (6æ—¥å‰)
ä¿æŒ: 7æ—¥åˆ†

Weekly (Father):
â”œâ”€ backup-20240107-weekly (å…ˆé€±)
â”œâ”€ backup-20231231-weekly (2é€±å‰)
â”œâ”€ backup-20231224-weekly (3é€±å‰)
â””â”€ backup-20231217-weekly (4é€±å‰)
ä¿æŒ: 4é€±åˆ†

Monthly (Grandfather):
â”œâ”€ backup-202401-monthly (ä»Šæœˆ)
â”œâ”€ backup-202312-monthly (å…ˆæœˆ)
â”œâ”€ backup-202311-monthly (2ãƒ¶æœˆå‰)
â””â”€ ...
ä¿æŒ: 12ãƒ¶æœˆåˆ†
```

```bash
# GFSæ–¹å¼ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

# 1. æ—¥æ¬¡ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆæ¯æ—¥åˆå‰3æ™‚ã€7æ—¥ä¿æŒï¼‰
velero schedule create daily-backup \
  --schedule="0 3 * * *" \
  --ttl 168h

# 2. é€±æ¬¡ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆæ¯é€±æ—¥æ›œåˆå‰4æ™‚ã€4é€±ä¿æŒï¼‰
velero schedule create weekly-backup \
  --schedule="0 4 * * 0" \
  --ttl 672h

# 3. æœˆæ¬¡ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆæ¯æœˆ1æ—¥åˆå‰5æ™‚ã€12ãƒ¶æœˆä¿æŒï¼‰
velero schedule create monthly-backup \
  --schedule="0 5 1 * *" \
  --ttl 8760h
```

---

## 4. Multi-cluster DR

### 4.1 ã‚¢ã‚¯ãƒ†ã‚£ãƒ–-ãƒ‘ãƒƒã‚·ãƒ–æ§‹æˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Primary Cluster (ap-northeast-1)                     â”‚
â”‚                                                       â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚ â”‚ Production   â”‚      â”‚ Velero       â”‚              â”‚
â”‚ â”‚ Workloads    â”‚      â”‚ (Backup)     â”‚              â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                              â”‚                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â”‚ S3 Replication
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ S3 Bucket (Multi-region)                             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Primary: ap-northeast-1                          â”‚ â”‚
â”‚ â”‚ Replica: us-west-2                               â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â”‚ Restore (DRç™ºç”Ÿæ™‚)
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DR Cluster (us-west-2) - é€šå¸¸ã¯åœæ­¢                  â”‚
â”‚                                                       â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚ â”‚ Velero       â”‚ â”€â”€â”€â–º â”‚ Production   â”‚              â”‚
â”‚ â”‚ (Restore)    â”‚      â”‚ Workloads    â”‚              â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### DR Cluster ã¸ã®ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼æ‰‹é †

```bash
#!/bin/bash
# dr-failover.sh - DR ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¸ã®ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼

set -euo pipefail

echo "=== Disaster Recovery Failover Procedure ==="
echo "Primary Cluster: production-eks (ap-northeast-1)"
echo "DR Cluster: dr-eks (us-west-2)"
echo ""
read -p "Confirm failover to DR cluster? (type 'FAILOVER' to continue): " confirm

if [ "$confirm" != "FAILOVER" ]; then
  echo "Failover cancelled"
  exit 1
fi

# 1. DR ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«åˆ‡ã‚Šæ›¿ãˆ
echo "Step 1: Switching to DR cluster..."
kubectl config use-context dr-eks
kubectl cluster-info

# 2. Velero ãŒåˆ©ç”¨å¯èƒ½ã‹ç¢ºèª
echo "Step 2: Verifying Velero..."
kubectl get pods -n velero

# 3. æœ€æ–°ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ç¢ºèª
echo "Step 3: Finding latest backup..."
LATEST_BACKUP=$(velero backup get -o json | jq -r '.items | sort_by(.status.completionTimestamp) | last | .metadata.name')
echo "Latest backup: ${LATEST_BACKUP}"

velero backup describe ${LATEST_BACKUP}

# 4. ãƒªã‚¹ãƒˆã‚¢å®Ÿè¡Œ
echo "Step 4: Restoring from backup..."
velero restore create dr-restore-$(date +%Y%m%d-%H%M%S) \
  --from-backup ${LATEST_BACKUP} \
  --wait

# 5. Podã®èµ·å‹•ç¢ºèª
echo "Step 5: Waiting for pods to be ready..."
kubectl wait --for=condition=Ready pods --all -n prod --timeout=600s

# 6. ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
echo "Step 6: Health check..."
kubectl get pods --all-namespaces
kubectl get svc --all-namespaces

# 7. DNSã‚’åˆ‡ã‚Šæ›¿ãˆï¼ˆRoute53ä¾‹ï¼‰
echo "Step 7: Updating DNS..."
aws route53 change-resource-record-sets \
  --hosted-zone-id Z1234567890ABC \
  --change-batch '{
    "Changes": [{
      "Action": "UPSERT",
      "ResourceRecordSet": {
        "Name": "api.example.com",
        "Type": "CNAME",
        "TTL": 60,
        "ResourceRecords": [{"Value": "dr-cluster-alb.us-west-2.elb.amazonaws.com"}]
      }
    }]
  }'

echo ""
echo "=== Failover completed ==="
echo "Services are now running on DR cluster"
echo "Don't forget to:"
echo "1. Notify stakeholders"
echo "2. Monitor the DR cluster closely"
echo "3. Plan for failback when primary is restored"
```

### 4.2 ã‚¢ã‚¯ãƒ†ã‚£ãƒ–-ã‚¢ã‚¯ãƒ†ã‚£ãƒ–æ§‹æˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cluster 1 (ap-northeast-1)                           â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚ â”‚ Workloads    â”‚ â—„â”€â”                                 â”‚
â”‚ â”‚ (Active)     â”‚   â”‚ Multi-cluster Service Mesh     â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ (Istio Multi-primary)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cluster 2 (us-west-2)                                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                                 â”‚
â”‚ â”‚ Workloads    â”‚ â—„â”€â”˜                                 â”‚
â”‚ â”‚ (Active)     â”‚                                     â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Global Load Balancer (AWS Global Accelerator / Cloudflare)
              â”‚
              â”œâ”€â”€â–º Cluster 1 (70% traffic)
              â””â”€â”€â–º Cluster 2 (30% traffic)
```

---

## 5. DRè¨“ç·´ã®å®Ÿæ–½

### 5.1 DRè¨“ç·´ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

```markdown
# DRè¨“ç·´å®Ÿæ–½è¨ˆç”»æ›¸

## è¨“ç·´æ¦‚è¦
- **ç›®çš„**: ãƒ—ãƒ©ã‚¤ãƒãƒªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼éšœå®³æ™‚ã®DRã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¸ã®ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼æ‰‹é †ã®æ¤œè¨¼
- **æ—¥æ™‚**: 2024å¹´2æœˆ15æ—¥ 22:00-24:00
- **å¯¾è±¡ã‚·ã‚¹ãƒ†ãƒ **: æœ¬ç•ªç’°å¢ƒï¼ˆproduction-eksï¼‰
- **è¨“ç·´æ–¹å¼**: ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå®Ÿéš›ã®ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼ã¯å®Ÿæ–½ã—ãªã„ï¼‰

## äº‹å‰æº–å‚™ï¼ˆ1é€±é–“å‰ï¼‰
- [ ] DRè¨“ç·´å‚åŠ è€…ã®ç¢ºå®š
- [ ] DRæ‰‹é †æ›¸ã®æœ€æ–°åŒ–
- [ ] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®æœ€æ–°æ€§ç¢ºèª
- [ ] DRã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®èµ·å‹•ç¢ºèª
- [ ] ç›£è¦–ãƒ„ãƒ¼ãƒ«ã®æº–å‚™
- [ ] ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ã¸ã®é€šçŸ¥

## è¨“ç·´å½“æ—¥ï¼ˆé–‹å§‹å‰ï¼‰
- [ ] å…¨å‚åŠ è€…ã®æ¥ç¶šç¢ºèª
- [ ] æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ç¢ºèª
- [ ] DRã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ãƒªã‚½ãƒ¼ã‚¹ç¢ºèª
- [ ] é€šä¿¡ç’°å¢ƒã®ç¢ºèª

## è¨“ç·´æ‰‹é †
### ãƒ•ã‚§ãƒ¼ã‚º1: éšœå®³æ¤œçŸ¥ï¼ˆ22:00-22:10ï¼‰
- [ ] ãƒ—ãƒ©ã‚¤ãƒãƒªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼éšœå®³ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
- [ ] ã‚¢ãƒ©ãƒ¼ãƒˆç™ºç«ã®ç¢ºèª
- [ ] ã‚ªãƒ³ã‚³ãƒ¼ãƒ«æ‹…å½“è€…ã¸ã®é€šçŸ¥
- [ ] ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå®£è¨€

### ãƒ•ã‚§ãƒ¼ã‚º2: åˆå‹•å¯¾å¿œï¼ˆ22:10-22:20ï¼‰
- [ ] ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œãƒãƒ¼ãƒ ã®æ‹›é›†
- [ ] çŠ¶æ³ã®æŠŠæ¡ã¨å½±éŸ¿ç¯„å›²ã®ç‰¹å®š
- [ ] DRç™ºå‹•ã®æ„æ€æ±ºå®š
- [ ] ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ã¸ã®å ±å‘Š

### ãƒ•ã‚§ãƒ¼ã‚º3: DRå®Ÿè¡Œï¼ˆ22:20-23:00ï¼‰
- [ ] DRã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¸ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ‡ã‚Šæ›¿ãˆ
- [ ] æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ç‰¹å®š
- [ ] ãƒªã‚¹ãƒˆã‚¢ã®å®Ÿè¡Œ
- [ ] Podèµ·å‹•çŠ¶æ…‹ã®ç¢ºèª
- [ ] ã‚µãƒ¼ãƒ“ã‚¹ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
- [ ] DNSåˆ‡ã‚Šæ›¿ãˆï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰

### ãƒ•ã‚§ãƒ¼ã‚º4: æ¤œè¨¼ï¼ˆ23:00-23:30ï¼‰
- [ ] å„ã‚µãƒ¼ãƒ“ã‚¹ã®å‹•ä½œç¢ºèª
- [ ] ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®ç¢ºèª
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
- [ ] å¤–éƒ¨é€£æºã®ç¢ºèª

### ãƒ•ã‚§ãƒ¼ã‚º5: æŒ¯ã‚Šè¿”ã‚Šï¼ˆ23:30-24:00ï¼‰
- [ ] å®Ÿæ–½å†…å®¹ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼
- [ ] å•é¡Œç‚¹ã®æ´—ã„å‡ºã—
- [ ] æ”¹å–„äº‹é …ã®è¨˜éŒ²
- [ ] æ¬¡å›è¨“ç·´æ—¥ç¨‹ã®ç¢ºèª

## æˆåŠŸåŸºæº–
- [ ] RTO: 40åˆ†ä»¥å†…ã«ãƒªã‚¹ãƒˆã‚¢å®Œäº†ï¼ˆç›®æ¨™: 60åˆ†ï¼‰
- [ ] RPO: ãƒ‡ãƒ¼ã‚¿æå¤±5åˆ†ä»¥å†…ï¼ˆç›®æ¨™: 15åˆ†ï¼‰
- [ ] ã™ã¹ã¦ã®Criticalã‚µãƒ¼ãƒ“ã‚¹ãŒæ­£å¸¸ç¨¼åƒ
- [ ] æ‰‹é †æ›¸ã©ãŠã‚Šã«å®Ÿæ–½ã§ããŸ

## è¨“ç·´å¾Œå¯¾å¿œ
- [ ] è¨“ç·´ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ
- [ ] DRæ‰‹é †æ›¸ã®æ›´æ–°
- [ ] æ”¹å–„äº‹é …ã®å¯¾å¿œ
- [ ] ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ã¸ã®å ±å‘Š
```

### 5.2 è¨“ç·´è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
#!/bin/bash
# dr-drill.sh - DRè¨“ç·´ã®è‡ªå‹•å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

set -euo pipefail

DRILL_DATE=$(date +%Y%m%d-%H%M%S)
REPORT_FILE="dr-drill-report-${DRILL_DATE}.md"

echo "=== DR Drill Started at ${DRILL_DATE} ===" | tee ${REPORT_FILE}

# ã‚¿ã‚¤ãƒãƒ¼é–‹å§‹
START_TIME=$(date +%s)

# Phase 1: éšœå®³æ¤œçŸ¥
echo "" | tee -a ${REPORT_FILE}
echo "## Phase 1: Failure Detection" | tee -a ${REPORT_FILE}
PHASE1_START=$(date +%s)

echo "Simulating primary cluster failure..." | tee -a ${REPORT_FILE}
# ï¼ˆå®Ÿéš›ã®è¨“ç·´ã§ã¯ã€ãƒ—ãƒ©ã‚¤ãƒãƒªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä¸€æ™‚çš„ã«é®æ–­ã™ã‚‹ç­‰ï¼‰

PHASE1_END=$(date +%s)
PHASE1_DURATION=$((PHASE1_END - PHASE1_START))
echo "Phase 1 completed in ${PHASE1_DURATION} seconds" | tee -a ${REPORT_FILE}

# Phase 2: DRå®Ÿè¡Œ
echo "" | tee -a ${REPORT_FILE}
echo "## Phase 2: DR Execution" | tee -a ${REPORT_FILE}
PHASE2_START=$(date +%s)

# DRã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«åˆ‡ã‚Šæ›¿ãˆ
echo "Switching to DR cluster..." | tee -a ${REPORT_FILE}
kubectl config use-context dr-eks

# æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å–å¾—
LATEST_BACKUP=$(velero backup get -o json | jq -r '.items | sort_by(.status.completionTimestamp) | last | .metadata.name')
echo "Latest backup: ${LATEST_BACKUP}" | tee -a ${REPORT_FILE}

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®è©³ç´°ã‚’è¨˜éŒ²
velero backup describe ${LATEST_BACKUP} >> ${REPORT_FILE}

# ãƒªã‚¹ãƒˆã‚¢å®Ÿè¡Œ
RESTORE_NAME="dr-drill-${DRILL_DATE}"
echo "Starting restore: ${RESTORE_NAME}..." | tee -a ${REPORT_FILE}
velero restore create ${RESTORE_NAME} \
  --from-backup ${LATEST_BACKUP} \
  --wait

PHASE2_END=$(date +%s)
PHASE2_DURATION=$((PHASE2_END - PHASE2_START))
echo "Phase 2 completed in ${PHASE2_DURATION} seconds" | tee -a ${REPORT_FILE}

# Phase 3: æ¤œè¨¼
echo "" | tee -a ${REPORT_FILE}
echo "## Phase 3: Verification" | tee -a ${REPORT_FILE}
PHASE3_START=$(date +%s)

# PodçŠ¶æ…‹ã®ç¢ºèª
echo "Checking pod status..." | tee -a ${REPORT_FILE}
kubectl get pods --all-namespaces -o wide >> ${REPORT_FILE}

# ã‚µãƒ¼ãƒ“ã‚¹ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
echo "Running health checks..." | tee -a ${REPORT_FILE}
HEALTH_CHECK_FAILED=0

# å„ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
for service in api frontend backend; do
  echo "Checking ${service}..." | tee -a ${REPORT_FILE}
  if kubectl exec -n prod deploy/${service} -- curl -f http://localhost:8080/health; then
    echo "âœ“ ${service} is healthy" | tee -a ${REPORT_FILE}
  else
    echo "âœ— ${service} health check failed" | tee -a ${REPORT_FILE}
    HEALTH_CHECK_FAILED=1
  fi
done

PHASE3_END=$(date +%s)
PHASE3_DURATION=$((PHASE3_END - PHASE3_START))
echo "Phase 3 completed in ${PHASE3_DURATION} seconds" | tee -a ${REPORT_FILE}

# ç·åˆè©•ä¾¡
END_TIME=$(date +%s)
TOTAL_DURATION=$((END_TIME - START_TIME))
RTO_TARGET=3600  # 60åˆ†

echo "" | tee -a ${REPORT_FILE}
echo "## Summary" | tee -a ${REPORT_FILE}
echo "- Total duration: ${TOTAL_DURATION} seconds ($((TOTAL_DURATION / 60)) minutes)" | tee -a ${REPORT_FILE}
echo "- RTO target: ${RTO_TARGET} seconds ($((RTO_TARGET / 60)) minutes)" | tee -a ${REPORT_FILE}

if [ ${TOTAL_DURATION} -le ${RTO_TARGET} ] && [ ${HEALTH_CHECK_FAILED} -eq 0 ]; then
  echo "- Result: âœ“ SUCCESS" | tee -a ${REPORT_FILE}
  EXIT_CODE=0
else
  echo "- Result: âœ— FAILED" | tee -a ${REPORT_FILE}
  EXIT_CODE=1
fi

echo "" | tee -a ${REPORT_FILE}
echo "Report saved to: ${REPORT_FILE}"

exit ${EXIT_CODE}
```

---

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ç½å®³å¾©æ—§ã®ä¸»è¦éƒ¨åˆ†ã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã¾ã™ã€‚æ¬¡ã¯ã€Œã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é‹ç”¨ã€ã¨ã€Œãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã€ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

## ğŸ“š å‚è€ƒãƒªã‚½ãƒ¼ã‚¹

- [etcd Disaster Recovery](https://etcd.io/docs/latest/op-guide/recovery/)
- [Velero Documentation](https://velero.io/docs/)
- [AWS Backup for EKS](https://docs.aws.amazon.com/eks/latest/userguide/backup-restore.html)
- [Kubernetes Disaster Recovery Best Practices](https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster)
